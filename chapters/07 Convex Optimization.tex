\section{Convex Optimization}

Convex optimization studies problems whose objective and feasible sets are convex, guaranteeing that every local optimum is also global. These structure-driven guarantees let us design algorithms with provable convergence, quantify sensitivity, and provide certificates of optimality via dual variables. This chapter outlines the key definitions, canonical problem classes, duality theory, and foundational algorithms emphasized in the lecture notes.

\subsection{Convex sets, functions, and problems}
Let $C \subseteq \mathbb{R}^d$ be a set. $C$ is \textbf{convex} if for any $x, y \in C$ and $\theta \in [0,1]$, the convex combination $\theta x + (1-\theta) y$ lies in $C$. Typical convex sets are affine subspaces, halfspaces, Euclidean balls, ellipsoids, spectrahedra, and probability simplices.

A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is \textbf{convex} if its domain is convex and for every $x, y$ and $\theta \in [0,1]$
\[
    f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y).
\]
When $f$ is differentiable, convexity is equivalent to the first-order inequality
\[
    f(y) \ge f(x) + \nabla f(x)^\top (y - x),
\]
meaning the affine tangent is a global under-estimator. If $f$ is twice differentiable, convexity is equivalent to $\nabla^2 f(x) \succeq 0$ for all $x$ in the interior of the domain. \textbf{Strong convexity} with parameter $m>0$ strengthens the inequality to
\[
    f(y) \ge f(x) + \nabla f(x)^\top (y-x) + \frac{m}{2}\|y-x\|^2,
\]
implying a unique minimizer and improved convergence rates.

A constrained optimization problem minimizes an objective subject to equality and inequality constraints. It is convex when the objective and inequality constraints are convex and the equality constraints are affine, so the feasible set is convex.

A generic convex optimization problem reads
\[
\begin{aligned}
    \min_{x \in \mathbb{R}^d}\quad & f_0(x) \\
    \text{s.t.}\quad & f_i(x) \le 0,\quad i = 1,\ldots,m, \\
    & Ax = b,
\end{aligned}
\]
where each $f_i$ is convex and the equality constraints define an affine set. The feasible set $\{x \mid f_i(x) \le 0, Ax=b\}$ must be nonempty for the problem to be well-posed. Convexity ensures that any Karush--Kuhn--Tucker (KKT) point is globally optimal.

\subsection{Illustrative example: closest point on a disk}
Consider a drone at $(x_0, y_0, z_0)$ and a person restricted to the flat disk $\{(x, y, 0) \mid x^2 + y^2 \le r\}$. The closest point solves
\[
\min_{x,y,z}\ \tfrac{1}{2}\|(x, y, z) - (x_0, y_0, z_0)\|_2^2 \quad \text{s.t.}\quad z = 0,\; x^2 + y^2 \le r.
\]
Let $f(x, y, z) = \tfrac{1}{2}\|(x, y, z) - (x_0, y_0, z_0)\|_2^2$, $g(x, y, z) = z$, and $h(x, y, z) = x^2 + y^2 - r$. Then
\begin{align*}
    \nabla f(x, y, z) &= (x - x_0, y - y_0, z - z_0), \\
    \nabla g(x, y, z) &= (0, 0, 1), \\
    \nabla h(x, y, z) &= (2x, 2y, 0).
\end{align*}
At an optimum, there exist multipliers $\lambda \in \mathbb{R}$ and $\alpha \ge 0$ such that
\[
    \nabla f(x, y, z) + \lambda \nabla g(x, y, z) + \alpha \nabla h(x, y, z) = 0,
\]
together with $z=0$, $x^2 + y^2 \le r$, and $\alpha (x^2 + y^2 - r) = 0$. This captures whether the closest point lies in the interior of the disk ($\alpha = 0$) or on the boundary ($\alpha > 0$).

\subsection{Standard convex programs in machine learning}
Many estimators from earlier chapters fit this template:
\begin{itemize}
    \item \textbf{Least squares / ridge regression:} $f(x)=\tfrac{1}{2}\|Ax - b\|_2^2 + \tfrac{\lambda}{2}\|x\|_2^2$ is convex quadratic; ridge adds strong convexity, yielding the closed form in equation \eqref{eq:map-ridge}.
    \item \textbf{Lasso:} minimize $\tfrac{1}{2}\|Ax-b\|_2^2 + \lambda \|x\|_1$. The $\ell_1$ norm promotes sparsity via a polyhedral penalty while keeping the problem convex. Soft-thresholding is the proximal operator driving coordinate descent.
    \item \textbf{Support Vector Machines (SVM):} hinge-loss objectives $\sum_i \max(0, 1 - y_i w^\top x_i)$ with $\ell_2$ regularization define convex problems whose dual has sparse support vectors.
    \item \textbf{Logistic regression:} the negative log-likelihood $\sum_i \log(1 + \exp(-y_i w^\top x_i))$ is convex. Adding $\ell_1$ or $\ell_2$ penalties yields generalized linear models solvable via gradient or Newton methods.
    \item \textbf{Matrix completion:} minimizing $\sum_{(i,j)\in\Omega}(X_{ij} - M_{ij})^2 + \lambda \|X\|_\ast$ uses the nuclear norm (convex surrogate of rank) to recover low-rank matrices.
\end{itemize}

Specialized subfamilies often admit faster algorithms:
\begin{itemize}
    \item \textbf{Linear programs (LP):} $f_0(x) = c^\top x$, $f_i(x) = a_i^\top x - b_i$.
    \item \textbf{Quadratic programs (QP):} quadratic objective with linear constraints.
    \item \textbf{Second-order cone programs (SOCP)} and \textbf{semidefinite programs (SDP)} capture norms and PSD constraints, respectively, and power robust control plus covariance fitting.
\end{itemize}

\subsection{Lagrangian duality and KKT conditions}
For
\[
\min_x f_0(x) \quad \text{s.t.}\quad f_i(x) \le 0,\, Ax = b,
\]
introduce multiplier $\lambda \ge 0$ for inequalities and $\nu$ for equalities. The \textbf{Lagrangian} is
\[
    \mathcal{L}(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \nu^\top (Ax - b)
\]
Intuitively, it adds weighted penalties for constraint violations to the objective: $\lambda$ (with $\lambda \ge 0$) acts like a price for violating $f_i(x) \le 0$, while $\nu$ enforces the equality $Ax=b$. The saddle-point view is that we minimize over $x$ but maximize over multipliers ($\lambda, \nu$), so any violation is punished at the optimum, yielding feasibility and the KKT conditions.

The Lagrangian lower-bounds the primal objective: $g(\lambda, \nu) = \inf_x \mathcal{L}(x, \lambda, \nu) \le f_0(x)$ for all feasible $x$. The \textbf{dual problem} maximizes $g(\lambda,\nu)$ subject to $\lambda \ge 0$. Weak duality ($g \le p^\star$) always holds, where $p^\star$ is the optimal primal value, while \textbf{strong duality} ($g^\star = p^\star$) holds under mild constraint qualifications such as Slater's condition (strict feasibility: there exists $x$ with $f_i(x) < 0$ and $Ax=b$). Dual variables often have sensitivity interpretations, e.g., the effect of tightening constraints. The dual trades the original constraints for a typically more complex objective but a simpler feasible set, which can make problems like SVMs easier to solve.

The \textbf{KKT conditions} describe optimality when strong duality holds:
\begin{align*}
    &\text{Primal feasibility: } f_i(x^\star) \le 0,\; Ax^\star = b. \\
    &\text{Dual feasibility: } \lambda^\star \ge 0. \\
    &\text{Stationarity: } \nabla f_0(x^\star) + \sum_i \lambda_i^\star \nabla f_i(x^\star) + A^\top \nu^\star = 0. \\
    &\text{Complementary slackness: } \lambda_i^\star f_i(x^\star) = 0.
\end{align*}
For unconstrained problems these reduce to $\nabla f_0(x^\star) = 0$, consistent with calculus. In SVMs, for example, KKT implies that only points on the margin have non-zero dual variables, explaining sparsity in the dual solution.

\subsection{Solving convex optimization problems}
When Slater's condition holds, the KKT conditions are necessary and sufficient. A direct strategy is to solve the KKT system (stationarity, primal feasibility, dual feasibility, and complementary slackness) for $x^\star$, $\lambda^\star$, and $\nu^\star$. If this system is intractable, solve the dual problem instead; under strong duality, primal solutions can be recovered from the dual optimum using stationarity and complementary slackness.
