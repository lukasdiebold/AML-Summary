\section{Ensemble Methods}

Bagging, boosting, and stacking are the three canonical ensemble strategies:
\begin{itemize}
    \item \textbf{Bagging} (bootstrap aggregating) trains many models independently on bootstrapped samples and averages or votes their predictions to reduce variance.
    \item \textbf{Boosting} trains models sequentially, each focusing on the errors of the previous ones, to reduce bias (e.g., AdaBoost, gradient boosting).
    \item \textbf{Stacking} trains diverse base models and then fits a meta-model on their outputs to learn the best combination.
\end{itemize}

Ensemble methods combine multiple base learners to obtain a predictor whose bias, variance, or loss surface is superior to any constituent model. By averaging or sequentially correcting learners trained on diverse views of the data, ensembles produce more stable predictions, calibrated uncertainties, and richer inductive biases than single models. This chapter summarizes the bootstrap-based family (bagging and random forests), boosting, and more general stacking strategies, with an emphasis on how they target different points along the bias--variance trade-off.

\subsection{Why ensembles help}
Let $\hat f(x)$ be a single hypothesis trained on data $D$. Ensembles form $\hat f_{\text{ens}}(x) = \sum_{b=1}^B w_b \hat f_b(x)$ from base learners $\hat f_b$. Two canonical effects explain their success:
\begin{itemize}
    \item \textbf{Variance reduction.} If the $\hat f_b$'s are identically distributed with variance $\sigma^2$ and pairwise correlation $\rho$, then
    \[
        \operatorname{Var}\bigl[\hat f_{\text{avg}}(x)\bigr] = \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2,
    \]
    which shrinks as $B$ increases whenever $\rho < 1$. Bagging manipulates the data (bootstrap samples) to drive correlations down and thus stabilize high-variance learners such as decision trees or neural networks trained on small data.
    \begin{derivation}
    For the average predictor $\hat f_{\text{avg}}(x) = \tfrac{1}{B} \sum_{b=1}^B \hat f_b(x)$,
    \[
        \begin{aligned}
        \operatorname{Var}[\hat{f}(x)] & =\mathbb{E}_D\left(\hat{f}(X)-\mathbb{E}_D \hat{f}(X)\right)^2 \\
        & =\mathbb{E}_D\left(\frac{1}{B} \sum_{i=1}^B \hat{f}_i(x)-\frac{1}{B} \sum_{i=1}^B \mathbb{E}_D \hat{f}_i(x)\right)^2 \\
        & =\mathbb{E}_D\left(\frac{1}{B} \sum_{i=1}^B\left(\hat{f}_i(x)-\mathbb{E}_D \hat{f}_i(x)\right)\right)^2 \\
        & =\frac{1}{B^2} \sum_{i=1}^B \operatorname{Var}_D\left[\hat{f}_i(x)\right]+\frac{1}{B^2} \sum_{i \neq j} \operatorname{Cov}\left[\hat{f}_i(x), \hat{f}_j(x)\right]
        \end{aligned}
    \]
    If all base learners share variance $\sigma^2$ and pairwise covariance $\operatorname{Cov}(\hat f_i, \hat f_j) = \rho\,\sigma^2$,
    \begin{align*}
    \operatorname{Var}\bigl[\hat f_{\text{avg}}(x)\bigr]
    &= \frac{B}{B^2} \sigma^2 + \frac{B(B-1)}{B^2} \, \rho\,\sigma^2 \\
    &= \frac{1}{B}\,\sigma^2 + \left(1 - \frac{1}{B}\right) \rho\,\sigma^2 \\
    &= \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2.
    \end{align*}
    Thus variance decreases like $1/B$ when correlations are small ($\rho \approx 0$), while residual correlation limits the gain.
    \end{derivation}
    \item \textbf{Bias correction.} Sequential ensembles such as boosting fit a series of weak learners to the residuals (negative gradients) of the current model. Each stage nudges the predictor toward lower bias and can transform a barely better-than-random base learner into a strong classifier.
\end{itemize}
These mechanisms are complementary: bagging primarily attacks variance, boosting primarily attacks bias, and stacking blends heterogeneous models to capture complementary inductive biases.

\subsection{Bagging and random forests}
Bagging (bootstrap aggregating) trains each base learner on a bootstrap sample of the training set. For classification, the ensemble prediction is a majority vote; for regression, it is an average:
\[
\hat c_B(x) = \operatorname{sgn}\!\left(\sum_{b=1}^B c_b(x)\right), \qquad
\hat f_B(x) = \frac{1}{B}\sum_{b=1}^B f_b(x).
\]
Bootstrap samples overlap but are not identical, so high-variance learners (e.g., trees) move in different directions and averaging stabilizes them.

\begin{algorithmic}
\State \textbf{Bagging classifier}
\State Input: data $\{(x_i, y_i)\}_{i=1}^n$, number of models $B$
\For{$b = 1$ to $B$}
    \State draw bootstrap sample $Z_b$ from the data (sampling with replacement)
    \State fit classifier $c_b$ on $Z_b$
\EndFor
\State output $\hat c_B(x) = \operatorname{sgn}(\sum_{b=1}^B c_b(x))$
\end{algorithmic}

Random forests add a second source of randomness by selecting a random subset of features at each split. This further reduces correlation between trees without increasing bias too much. Each tree is a standard decision tree trained on its own bootstrap sample; at prediction time the forest aggregates all trees. Typical split criteria include Gini impurity, entropy, and misclassification rate.

\textbf{Weak learners used in ensembles.} In practice, weak learners can be decision stumps, full decision trees, perceptrons/MLPs, or radial basis function networks. Bagging benefits most from unstable learners whose predictions change noticeably under small perturbations of the data.

\subsection{Boosting}
For now we're only interested in classification.

Given a Dataset $D = \{(x_i, y_i)\}_{i=1} \subseteq \mathbb R^d \times \{-1, +1\} \sim p_*$, where $y_i$ are labels and $x_i$ are features, with an unknown distribution $p_*$, we want to learn a classifier $G: \mathbb R^d \to \{-1, +1\}$ that minimizes the expected 0--1 loss:
\[L(G) = \mathbb E_{(X, Y) \sim p_*}[\mathbb I \{G(X) \neq Y\}]\]
there are three problems with this formulation:
\begin{itemize}
    \item The distribution $p_*$ is unknown, so we cannot compute the expected loss directly.
    \item The 0--1 loss is not differentiable, making optimization difficult.
    \item The hypothesis space of all classifiers $G: \mathbb R^d \to \{-1, +1\}$ is too large to search over.
\end{itemize}
To address these issues, we make the following changes:
\begin{itemize}
    \item We replace the expected loss with the empirical loss on the training data:
    \[\hat L(G, D) = \frac{1}{n} \sum_{i=1}^n \mathbb I \{G(x_i) \neq y_i\}\]
    \item We replace the 0--1 loss with a surrogate loss function $\ell: \mathbb R \to \mathbb R_+$ that is differentiable and convex, such as the exponential loss $\ell(z) = e^{-yG(x)}$.
    \item We restrict our hypothesis space to a set of weak learners $\mathcal H^A$ ($A$ for additive), with
    \[
        \mathcal H^A = \left\{G_m \mid G_m(x) = \sum_{i\leq m} \beta_i f_i(x), f_i \in \mathcal H, \beta_i \in \mathbb R\right\}
    \]
\end{itemize}
This won't work with linear models. But with weak learners (e.g., decision stumps), we can use boosting to iteratively build a strong classifier by adding weak learners that minimize the surrogate loss on the training data. The additive form also makes it natural to update the model stage by stage.

So then our objective becomes:
\[
\min_{G \in \mathcal H^A} \frac{1}{n} \sum_{i\leq n}\mathcal L(y_i; G(x_i))
\]
with 
\[
    \mathcal L(y_i; G(x)) = \exp(-y_i G(x_i))
    = \exp(-y_i G_{m-1}(x_i))\exp(-y_i\beta_m f_m(x_i))
    = w_i^{m-1} \mathcal L(y_i; \beta_m f_m(x_i))
\]
because $G_m(x)=G_{m-1}(x)+\beta_m f_m(x)$. What did we do? We decomposed the loss at step $m$ into the loss at step $m-1$ and the contribution of the new weak learner $f_m$ with weight $\beta_m$. So now we have two things to optimize: the weak learner $f_m$ and its weight $\beta_m$. 

and we can rewrite the objective as:
\[
\min_{G_{m-1} \in \mathcal H^A_{m-1}} \min_{f_m \in \mathcal H, \beta_m \in \mathbb R} \sum_{i\leq n} w_i^{m-1} \mathcal L(y_i; \beta_m f_m(x_i))
\]

\subsection{Forward Stagewise Additive Modeling (FSAM)} 
FSAM provides the bridge from the additive objective to an actual algorithm. Instead of solving for all $f_m$ and $\beta_m$ at once, it builds the model stage by stage: each step adds the single component that most improves the current objective. This is both computationally simple and conceptually aligned with boosting's idea of sequential correction. At each step $m$, we fix the previous model $G_{m-1}$ and optimize for the new weak learner $f_m$ and its weight $\beta_m$:

\begin{algorithmic}
\State $G_0(x) \gets 0$, $w_i^0 \gets 1/n$ for all $i$
\For{$m = 1$ to $M$}
    \State $\min_{f_m \in \mathcal H, \beta_m \in \mathbb R} \sum_{i\leq n} w_i^{m-1} \mathcal L(y_i; \beta_m f_m(x_i))$
    \State $G_m(x) \gets G_{m-1}(x) + \beta_m f_m(x)$
    \State $w_i^m \propto w_i^{m-1} \exp(-y_i\beta_m f_m(x_i))$
\EndFor
\end{algorithmic}
Future classifiers try to correct the mistakes of past classifiers by focusing more on misclassified examples (increasing their weights). This way, the ensemble learns from its errors and improves over time.
Gradient boosting generalizes this stagewise procedure by choosing updates from the negative gradient of an arbitrary differentiable loss.

\subsection{Gradient Descent Boosting}
FSAM still depends on a particular loss and reweighting scheme. Gradient boosting generalizes the same stagewise idea to any differentiable loss by viewing boosting as gradient descent in function space. The key move is to replace the weighted classification objective with a gradient step on the loss with respect to the current model's predictions. Wanted $G_{\theta}(x)$ where $\theta \in \Theta$ are parameters. We can use gradient descent to minimize the empirical loss:

\begin{algorithmic}
\State $\theta_0$ given
\For{$t=1$ to $T$}
    \State $\theta_t \leftarrow \theta_{t-1}-\alpha_t \nabla_\theta \mathcal{L}\left(D ; \theta_{t-1}\right)$
\EndFor
\end{algorithmic}
At the end
\[
\theta_T = \theta_0 - \sum_{t\leq T} \alpha _t \nabla_\theta \mathcal L(D; \theta_{t-1}).
\]
The combined algorithm looks like this when we perform gradient descent in function space and fit weak learners to the negative gradients (pseudo-residuals). This reveals why boosting works for a wide range of losses and why the base learner only needs to approximate the negative gradient at each step:
\begin{algorithmic}
\State $G_0(x) \gets \arg\min_\gamma \sum_i \mathcal L(y_i; \gamma)$
\For{$t = 1$ to $m$}
    \State $r_i^{t-1} \gets -\left[\frac{\partial \mathcal L(y_i; G(x_i))}{\partial G(x_i)}\right]_{G(x)=G_{t-1}(x)}$ for all $i$
    \State $\min_{f_t \in \mathcal H, \beta_t \in \mathbb R} \sum_{i\leq n} (r_i^{t-1} - \beta_t f_t(x_i))^2$
    \State $G_t(x) \gets G_{t-1}(x) + \beta_t f_t(x)$
\EndFor
\end{algorithmic}
This recovers gradient boosting: each stage fits a weak learner to the pseudo-residuals and updates the additive model. Small step sizes and shallow trees typically improve generalization.
With exponential loss and binary weak learners, this reduces to AdaBoost.

\subsection{AdaBoost}
AdaBoost is the classic instantiation of the above ideas: it chooses the exponential loss, uses binary weak learners, and yields simple closed-form updates. The reweighting scheme is not arbitrary; it is exactly what makes each new classifier focus on mistakes while keeping the additive model interpretable. AdaBoost is a concrete instance of gradient boosting (and FSAM) with exponential loss and binary labels. Let $y_i \in \{-1,+1\}$ and $c_m(x) \in \{-1,+1\}$. At step $m$, the weighted error is
\[
\varepsilon_m = \frac{\sum_{i=1}^n w_i\,\mathbb{I}\{c_m(x_i) \neq y_i\}}{\sum_{i=1}^n w_i}.
\]
The update weight is
\[
\alpha_m = \tfrac{1}{2}\log\!\left(\frac{1-\varepsilon_m}{\varepsilon_m}\right),
\]
and the data weights are updated by
\[
 w_i \leftarrow w_i \exp\!\bigl(-\alpha_m y_i c_m(x_i)\bigr), \qquad \sum_i w_i = 1.
\]
The final classifier is the sign of the weighted vote, $\hat c(x)=\operatorname{sgn}(\sum_m \alpha_m c_m(x))$.

\begin{derivation}
The choice of $\alpha_m$ follows from minimizing the weighted exponential loss for a fixed classifier $c_m$:
\[
\sum_i w_i \exp\!\bigl(-\alpha y_i c_m(x_i)\bigr)
= (1-\varepsilon_m)e^{-\alpha} + \varepsilon_m e^{\alpha}.
\]
Differentiating and setting to zero yields $-\,(1-\varepsilon_m)e^{-\alpha} + \varepsilon_m e^{\alpha}=0$, hence
\[
\alpha_m = \tfrac{1}{2}\log\!\left(\frac{1-\varepsilon_m}{\varepsilon_m}\right).
\]
\end{derivation}
The algorithm only requires each weak learner to perform slightly better than chance ($\varepsilon_m < 1/2$). If a learner is worse than chance, flipping its predictions reduces the error.

\subsection{Boosting as additive modeling}
Beyond the algorithm, it is useful to understand what boosting is optimizing in the population. This perspective ties the stagewise updates to statistical decision theory and explains why boosting often improves margins. AdaBoost can be viewed as fitting an additive model by minimizing the exponential loss
\[
J(F) = \mathbb{E}\bigl[\exp(-yF(x))\bigr].
\]
The minimizer of this loss is proportional to the log-odds of the class probabilities, so $F(x)$ estimates a scaled log posterior ratio.
\begin{derivation}
Condition on $x$ and write $p = \mathbb{P}(y=1 \mid x)$. Then
\[
\mathbb{E}\!\left[e^{-yF(x)} \mid x\right] = p\,e^{-F(x)} + (1-p)\,e^{F(x)}.
\]
Differentiating with respect to $F(x)$ and setting to zero gives
\[
-p\,e^{-F(x)} + (1-p)\,e^{F(x)} = 0
\quad\Rightarrow\quad
F(x) = \tfrac{1}{2}\log\!\left(\frac{p}{1-p}\right).
\]
Thus the optimal $F(x)$ is a log-odds function, linking AdaBoost to additive logistic regression.
\end{derivation}
This perspective explains why boosting often improves margins and why it can be interpreted as stagewise functional optimization.

\subsection{Stacking}
Stacking combines heterogeneous models by training a meta-learner on their predictions. To avoid information leakage, base-model predictions for the meta-learner are typically generated via cross-validation. This lets the meta-learner discover which models are reliable in different regions of the input space, often outperforming any single model or uniform averaging.
