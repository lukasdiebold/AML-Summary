\section{Regression}
\subsection{Act 1: High-dimensional regression is unstable}

We assume $X$ and $y$ are distributed according to a distribution $p_*$ (i.e. $X, y \sim p_*$), where the output follows a noisy linear model:
$$
y = f_*(x) + \varepsilon \quad \text{with } \varepsilon \sim \mathcal N(0, \sigma^2)
$$
Here $f_*$ is the true (unknown) regression function and $\varepsilon$ is additive Gaussian noise with variance $\sigma^2$. Our task is to estimate $f_*$ from training data $D = \{(x_i, y_i)\}_{i=1}^n \sim p_*$.

The problem in this form is not tractable because the space of all possible functions is too large. We therefore restrict ourselves to linear functions:
$$f_*(x) = \beta^\top x$$
where $\beta \in \mathbb{R}^d$ is a parameter vector. Given the Gaussian noise assumption, each observation has likelihood $p(y_i | x_i, \beta) = \mathcal{N}(\beta^\top x_i, \sigma^2)$. We solve for $\beta$ using Maximum Likelihood Estimation (MLE):
$$
\begin{aligned}
\widehat{\beta} &=\arg \max _{\beta \in \mathbb{R}^d} p(D \mid \beta)\\
&= \arg \max _{\beta} \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta^\top x_i)^2}{2\sigma^2}\right) \\
&= \arg \min _{\beta} \sum_{i=1}^n (y_i - \beta ^\top x_i)^2 \\
&= \arg \min _{\beta} \text{MSE}(D, \beta)
\end{aligned}
$$
Maximizing the log-likelihood is equivalent to minimizing the mean squared error (MSE). The closed-form solution depends on whether we have more features than samples or vice versa:
$$
\begin{aligned}
\widehat{\beta} &= (X^\top X)^{-1} X^\top y \quad \text{(when } d < n\text{, more samples than features)} \\
&= X^\top (X X ^\top )^{-1}y \quad \text{(when } d > n\text{, more features than samples)}
\end{aligned}
$$
These are algebraically equivalent by the Woodbury matrix identity. The first formula is the standard \textit{ordinary least squares (OLS)} estimator, where 
$$
X=\left[\begin{array}{c}
-x_1- \\
\vdots \\
-x_n-
\end{array}\right] \quad y=\left[\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}\right]
$$
This estimator has some interesting properties. It is unbiased and, by the Gauss-Markov Theorem, it is the best linear unbiased estimator (BLUE), i.e. it attains the smallest variance among all linear unbiased estimators.
Thus, from the formula
$$
\text{error} = \text{bias}^2 + \text{variance} + \text{noise}
$$
we find that this estimator is the one with the smallest error of all the unbiased estimators. Then why does no-one use this estimator? If we introduce a bit of bias, we can significantly reduce the variance.

To understand the instability, we analyze $\operatorname{Var}(\hat{\beta})$ using the singular value decomposition (SVD) $X = UDV^\top$, where $U \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{d \times d}$ are orthogonal, and $D$ is diagonal with singular values $D_{11} \geq D_{22} \geq \ldots \geq 0$. Plugging this into the OLS formula:
$$\hat{\beta} = (X^\top X)^{-1}X^\top y = (VD^\top U^\top UDV^\top)^{-1} VD^\top U^\top y = VD^{-1}U^\top y$$
Since $y = X\beta_* + \varepsilon$ where $\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$, and we multiply $y$ by the deterministic matrix $VD^{-1}U^\top$, the estimator $\hat{\beta}$ is also Gaussian. Its variance is:
$$\operatorname{Var}(\hat{\beta})=\operatorname{Var}(VD^{-1}U^\top y) = VD^{-1}U^\top \operatorname{Var}(y) U D^{-1}V^\top =\sigma^2 V D^{-2} V^{\top} =\sigma^2 \sum_{i \leq r} \frac{1}{D_{ii}^2} V_i V_i^{\top} $$
where $r = \text{rank}(X)$ and $V_i$ is the $i$-th column of $V$ (the $i$-th right singular vector).

\textbf{The problem:} In high-dimensional data, features are often correlated (e.g., pixel intensities in images, gene expressions). This makes $X$ close to low-rank, so several singular values $D_{ii}$ are very small. The variance contributions $1/D_{ii}^2$ then explode for these directions, causing massive instability in $\hat{\beta}$ even though it remains unbiased. Small noise in $y$ gets amplified enormously in directions with small singular values, leading to wild predictions on test data. 

\subsection{Act 2: Stability via Regularization}
\label{chap:regression:act2}
The solution to the variance blow-up is to introduce regularization, which adds a small amount of bias in exchange for a large reduction in variance. We can derive regularization naturally from a Bayesian perspective.

The typical process of \textbf{Bayesian inference} goes through the following stages:
\begin{enumerate}
    \item Prior $\beta \sim \mathcal N(0, \tau^2 I)$ — we assume $\beta$ is drawn from a Gaussian centered at zero with variance $\tau^2$. This encodes our belief that coefficients should not be too large.
    \item Likelihood $p(D | \beta) = \prod_i \mathcal{N}(y_i | \beta^\top x_i, \sigma^2)$ — same Gaussian noise model as before.
    \item Posterior (via Bayes' rule) $p(\beta | D) \propto p(\beta) p(D | \beta) \propto \exp \left(-\frac{1}{2 \sigma^2} \text{MSE}(D, \beta)-\frac{1}{2 \tau^2}\|\beta\|^2\right)$
\end{enumerate}
The posterior combines the likelihood (fit to data) with the prior (regularization). To derive the optimization objective, we use the fact that maximizing the posterior probability is equivalent to minimizing its negative logarithm. From Bayes' rule:
$$
p(\beta | D) \propto p(\beta) p(D | \beta) = \mathcal{N}(0, \tau^2 I) \cdot \prod_i \mathcal{N}(y_i | \beta^\top x_i, \sigma^2)
$$

Taking the negative log (up to constant terms):
$$
-\log p(\beta | D) \propto \underbrace{-\log p(\beta)}_{-\frac{1}{2\tau^2}\|\beta\|^2 + \text{const}} + \underbrace{-\log p(D|\beta)}_{-\sum_i \log \mathcal{N}(y_i | \beta^\top x_i, \sigma^2)} 
$$

For Gaussian distributions, $-\log \mathcal{N}(y | \mu, \sigma^2) = \frac{(y-\mu)^2}{2\sigma^2} + \text{const}$, so:
$$
-\log p(\beta | D) \propto \frac{1}{2\tau^2}\|\beta\|^2 + \frac{1}{2\sigma^2}\sum_i (y_i - \beta^\top x_i)^2
$$

Minimizing this gives the MAP (maximum a posteriori) estimate:
\begin{align*}
    \hat{\beta}_{\text{MAP}} &= \arg\min_{\beta} \left[ \frac{1}{2\sigma^2} \sum_i (y_i - \beta^\top x_i)^2 + \frac{1}{2\tau^2} \|\beta\|^2 \right] \\
    & =\arg \min _\beta\left[\sum_i\left(y_i-\beta^{\top} x_i\right)^2+\lambda\|\beta\|^2\right]
\end{align*}
This is precisely \textbf{ridge regression} with regularization parameter $\lambda = \sigma^2/\tau^2$. The $\ell^2$ penalty $\|\beta\|^2$ shrinks coefficients toward zero. If we instead use a Laplace prior $p(\beta) \propto \exp(-|\beta|/\tau)$ with heavier tails, we obtain \textbf{lasso regression} with an $\ell^1$ penalty $\|\beta\|_1$, which promotes sparsity.

The prior variance $\tau^2$ controls the bias-variance trade-off: small $\tau^2$ (big $\lambda$) means strong regularization (more bias, less variance), while large $\tau^2$ (small $\lambda$) recovers OLS (no bias, high variance). The MAP (maximum a posteriori) solution is:
\begin{equation}
    \label{eq:map-ridge}
    \hat{\beta}_{\text {MAP }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y
\end{equation}
\begin{derivation}
Start from the MAP/ridge objective in matrix form
\[
J(\beta) = \sum_i (y_i - x_i^{\top}\beta)^2 + \lambda\,\|\beta\|^2
\;=\; \|y - X\beta\|^2 + \lambda\, \beta^{\top}\beta.
\]
Differentiate and set the gradient to zero:
\begin{align*}
\nabla_\beta J(\beta)
&= -2 X^{\top}(y - X\beta) + 2\lambda\,\beta \\
&= 2\,(X^{\top}X + \lambda I)\beta - 2 X^{\top}y \,=\, 0.
\end{align*}
Thus the normal equations are $(X^{\top}X + \lambda I)\,\hat\beta = X^{\top}y$. For $\lambda>0$, the matrix $X^{\top}X + \lambda I$ is positive definite and hence invertible, which yields the closed form in \eqref{eq:map-ridge}.
\end{derivation}
Compare this to OLS: the regularization term $\lambda I$ is added to $X^\top X$ before inversion, preventing ill-conditioning. Using SVD again to analyze the variance:
\begin{equation}
    \label{eq:var-ridge}
    \text{Var}(\hat \beta_{\text {MAP }}) = \sigma^2 \sum_{i\leq r} \frac{D_{ii}^2}{\left(D_{ii}^2+\lambda\right)^2} V_i V_i^{\top}
\end{equation}
The key is the \textbf{shrinkage factor} $\frac{D_{ii}^2}{(D_{ii}^2+\sigma^2/\tau^2)^2}$. For large singular values ($D_{ii}^2 \gg \sigma^2/\tau^2$), this is close to $1/D_{ii}^2$ (like OLS). For small singular values ($D_{ii}^2 \ll \sigma^2/\tau^2$), the factor is approximately $\tau^4 D_{ii}^2/\sigma^4$, which decays much more slowly than $1/D_{ii}^2$. This prevents variance blow-up in the problematic low-variance directions, stabilizing the estimator at the cost of introducing bias (shrinking coefficients toward zero). 

\subsection{Act 3: Polynomial regression via kernels}
Now we change our assumption for $f_*(x)$ to allow for nonlinear functions. We model $f_*$ as a linear function in an infinite-dimensional feature space:
$$f_*(x) = \varphi(x)^\top \beta _*$$
where $\beta_* \in \mathbb{R}^{\infty}$ and $\varphi(x)$ maps each input to an infinite-dimensional polynomial feature representation:
$$
\varphi(X)=K_x\left(\frac{x_1^{\alpha_1} ... x_d^{\alpha_d}}{\sqrt{\alpha_{1}!\ldots \alpha_{d}!}}\right)_{\alpha \in \mathbb{N}^d}
$$
This includes all polynomial terms of all degrees. The normalization by factorials ensures the inner product has a clean closed form.

Remarkably, the inner product of two infinite-dimensional feature vectors yields the radial basis function (RBF) kernel. For $x, x^{\prime} \in \mathbb{R}^a$:
$$
\begin{aligned}
\varphi(x)^{\top} \varphi\left(x^{\prime}\right) & =K_{RBF}\left(x, x^{\prime}\right) \\
& =\exp \left(-\frac{1}{2} ||x-x^{\prime} ||^2\right)
\end{aligned}$$
This follows from the Taylor expansion of the exponential function. The RBF kernel measures similarity: it is 1 when $x = x'$ and decays as points move apart.

We still want to minimize the MSE, but now in the infinite-dimensional feature space:
$$
\begin{aligned}
\hat{\beta} & =\arg \min _{\beta \in \mathbb{R}^{\infty}} \frac{1}{n} \sum_{i \leq n}\left(y_i-\varphi\left(x_i\right)^{\top} \beta\right)^2 \\
& =\Phi^{\top}\left(\Phi \Phi^{\top}\right)^{-1} y
\end{aligned}
$$
where 
$$
\Phi=\left[\begin{array}{l}
\varphi(x)^{\top}  \\
\varphi\left(x_2\right)^{\top}\\
\varphi\left(x_n\right)^{\top}
\end{array}\right] \in \mathbb R^{n \times \infty}
$$
Despite $\beta$ living in infinite dimensions, the representer theorem guarantees the solution lies in the span of the training features, so we can work with the $n \times n$ Gram matrix $\Phi \Phi^{\top}$ instead of the infinite-dimensional feature space directly.

To make a prediction at test point $x_*$, we compute:
$$
\begin{aligned}
\hat y_* & =\varphi\left(x_*\right)^{\top} \hat{\beta} \\
& =\varphi\left(x_*\right)^{\top} \Phi^{\top}\left(\Phi \Phi^{\top}\right)^{-1} y\\
&= k(x_*)^{\top} K^{-1} y
\end{aligned}
$$
where $k(x_*) = \left(\varphi\left(x_*\right)^{\top} \varphi\left(x_i\right)\right)_{1 \leq i \leq n} = \left(K_{RBF}(x_*, x_i)\right)_{1 \leq i \leq n}$ is an $n$-dimensional vector of kernel evaluations between the test point and each training point, and $K_{ij}=\varphi(x_i)^{\top} \varphi(x_j) = K_{RBF}(x_i, x_j)$ is the $n \times n$ kernel matrix.

This is the \textbf{kernel trick}: we never explicitly construct the infinite-dimensional $\varphi(\cdot)$. Instead, we only compute inner products via the kernel function $K_{RBF}$, which can be evaluated in closed form. The prediction is a weighted combination of training outputs, where the weights depend on how similar the test point is to each training point.

The problem is that the inversion of the matrix is $O(n^3)$, which becomes costly for large datasets even though we avoided the infinite feature map explicitly.

\subsection{Act 4: Neural Networks}
We assume $f_*$ has only a single, very wide hidden layer.
$$f_*(X)=\frac{1}{\sqrt{m}} \sum_{i \leq m} \alpha_i \phi\left(\omega_i^{\top} X\right)$$
where $\phi$ is a nonlinear activation function (e.g. ReLU, tanh), and the network has $m$ hidden units. The parameters are $\theta=\left\{\alpha_i, w_i\right\}_{i \leq m}$, i.e. both the output weights $\alpha_i$ and the input weights $w_i$ are learned. We initialize with
$$
\theta_0 \sim \mathcal N\left(0, w^2\right)
$$
and we update our parameters using gradient descent. 
$$
\theta_{t+1} \leftarrow \theta_t-\eta \nabla_\theta \text { MSE}(D, \theta_t)
$$
The gradient can be written in matrix form as
$$\nabla_\theta \operatorname{MSE}\left(D, \theta_t\right)=\widetilde{\Phi}_t^{\top}\left(f_t-y\right)$$
where 
$$
\tilde{\Phi}_t=\left(-\nabla_\theta f\left(x_i ; \theta_t\right)^\top-\right)_{i \leq n} \in \mathbb{R}^{n \times |\theta|} \quad \text{ and } \quad f_t=\left(f(x_i ; \theta_t)\right)_{i \leq n} \in \mathbb{R}^n
$$
Here $\tilde{\Phi}_t$ is the feature matrix whose $i$-th row is the gradient of the network output with respect to all parameters, evaluated at data point $x_i$ and current parameters $\theta_t$.

In the \textit{lazy training regime} (small learning rate, wide network), the parameters stay close to initialization, so we can linearize the network via a first-order Taylor expansion around $\theta_0$:
$$f_t \approx f_0+\tilde \Phi_0\left(\theta_t-\theta_0\right)$$
Assuming the feature matrix $\tilde{\Phi}_t$ remains approximately constant at $\tilde{\Phi}_0$ (which holds when $m \to \infty$), gradient flow yields
$$
\theta_t-\theta_0=\tilde{\Phi}_0^{\top}\left(\tilde{\Phi}_0 \tilde{\Phi}_0^{\top}\right)^{-1}\left(f_t-f_0\right)
$$
This says the parameter change lies in the span of the gradients and is chosen to optimally fit the training residuals.

Now let $x_*$ be a test point. Plugging the linearization into the prediction yields
$$
f_t\left(x_*\right) \approx f_0\left(x_*\right)+\nabla_\theta f\left(x_*, \theta_0\right)^{\top} \tilde{\Phi}_0^{\top}\left(\tilde{\Phi}_0 \tilde{\Phi}_0^{\top}\right)^{-1}\left(f_t-f_0\right)
$$
Define the \textit{neural tangent kernel (NTK)} $K$ with entries
$$
K_{ij} = \nabla_\theta f(x_i, \theta_0)^{\top} \nabla_\theta f(x_j, \theta_0) = \left[\tilde{\Phi}_0 \tilde{\Phi}_0^{\top}\right]_{ij}
$$
and similarly $k(x_*) = \left(\nabla_\theta f(x_*, \theta_0)^{\top} \nabla_\theta f(x_i, \theta_0)\right)_{i \leq n}$. 

In the infinite-width limit ($m \to \infty$), the random initialization ensures $f_0(x) \to 0$ for all $x$ (the outputs average out), and after infinite training time ($t \to \infty$), gradient descent drives the training residual to zero so $f_t \to y$. The prediction becomes
$$
f_\infty\left(x_*\right) = k\left(x_*\right)^{\top} K^{-1} y
$$
This is exactly the result we obtained in Act 3 for kernel regression. 

\textbf{Conclusion:} Gradient descent on a very wide neural network operates in a \textit{kernel regime}, where training is equivalent to kernel ridge regression with the neural tangent kernel. The NTK is determined by the architecture and activation function, but the solution has the same closed-form structure $k(x_*)^{\top} K^{-1} y$ as any other kernel method. In practice, finite-width networks can escape this regime and learn richer, feature-learning representations—this lazy limit is a useful theoretical baseline.

\subsection{Some Things from the Slides}

\paragraph{MAP, ERM, and the conditional mean.} For squared loss and any hypothesis class rich enough to contain the regression function, the Bayes-optimal solution is the conditional expectation $f^\star(x) = \mathbb{E}[Y \mid X = x]$, i.e.
\[
f^\star \in \arg\min_f \mathbb{E}_{X,Y}\left[(Y - f(X))^2\right].
\]
In practice $P(Y \mid X)$ is unknown, so we either (i) postulate a parametric model and perform maximum likelihood / MAP (e.g., assume $Y \mid X \sim \mathcal{N}(f_\beta(X), \sigma^2)$ and maximize $\sum_i \log p(y_i \mid x_i, \beta)$) or (ii) minimize the empirical risk $\sum_i (y_i - f(x_i))^2$ directly. For well-behaved models these two routes coincide, which explains why classical ERM with squared loss reproduces the MAP estimator of a Gaussian noise model.

\paragraph{Gauss–Markov optimality.} Ordinary least squares does not merely give \emph{a} solution, it gives the best linear unbiased estimator (BLUE). Consider any linear estimator $\tilde{\theta} = c^\top y$ that is unbiased for $a^\top \beta$ (i.e., $\mathbb{E}[\tilde{\theta}] = a^\top \beta$). The Gauss–Markov theorem states
\[
\operatorname{Var}(a^\top \hat{\beta}) \leq \operatorname{Var}(\tilde{\theta}),
\]
where $\hat{\beta} = (X^\top X)^{-1}X^\top y$ is the OLS solution and $a$ is an arbitrary vector. Intuitively, any other unbiased linear estimator differs from OLS by an additive linear operator $D$ with $a^\top D X = 0$, which only inflates variance through the positive semi-definite term $D D^\top$. This reinforces the motivation for OLS (or its regularized cousins) when unbiasedness and linearity are desired.

\paragraph{Bias–variance decomposition.} Suppose we highlight a specific input $x$ and view the learned regressor $\hat{f}$ as a random variable (due to sampling various training sets). The expected squared prediction error decomposes into variance, bias, and irreducible noise:
\[
\mathbb{E}_D \mathbb{E}_{Y|X=x}\left[(\hat{f}(x) - Y)^2\right]
= \underbrace{\mathbb{E}_D\left[(\hat{f}(x) - \mathbb{E}_D[\hat{f}(x)])^2\right]}_{\text{variance}}
+ \underbrace{\left(\mathbb{E}_D[\hat{f}(x)] - \mathbb{E}[Y \mid X = x]\right)^2}_{\text{bias}^2}
+ \underbrace{\operatorname{Var}(Y \mid X = x)}_{\text{noise}}.
\]
Low-capacity models (small hypothesis classes) produce high bias and low variance, while expressive models produce the opposite. Managing this trade-off is the crux of regularization and model selection.

\paragraph{Shrinkage beyond ridge and lasso.} Ridge ($\ell_2$) and lasso ($\ell_1$) penalties are instances of a broader shrinkage family
\[
\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^d x_{i,j}\beta_j\right)^2 + \lambda \sum_{j=1}^d |\beta_j|^q,\quad q \in (0, \infty].
\]
Varying $q$ changes the geometry of the constraint set: $q=2$ yields spherical ridge contours, $q=1$ produces diamond-shaped lasso constraints whose corners encourage sparsity, and $q<1$ (non-convex) promotes even stronger sparsity at the cost of more difficult optimization. These shrinkage priors can be interpreted as MAP estimators with different prior distributions on $\beta$ (Gaussian, Laplace, etc.) and help calibrate the bias–variance compromise by shrinking noisy coefficients toward zero.
In practice we trace the coefficient paths as the tuning parameter (either $\lambda$ or the effective degrees of freedom) varies, and select the desired amount of shrinkage via cross-validation on held-out data. Ridge paths shrink smoothly without hitting zero, whereas lasso paths \emph{do} cross zero, enabling feature selection alongside regularization.
