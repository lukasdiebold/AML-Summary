\section{Support Vector Machines}

For support vector machines (SVMs), we're interested in maximising the margin between classes. So given a dataset $D=\{(x_i, y_i)\}$ with $y_i \in \{-1, +1\}$ and guaranteed linear separability, we want to find a hyperplane defined by $(w, w_0)$ such that $w^T x_i + w_0 > 0$ if $y_i = 1$ and $w^T x_i + w_0 < 0$ if $y_i = -1$. Or in other terms, we want to satisfy the constraints $y_i (w^T x_i + w_0) > 0$ for all $i$.
The margin is defined as the distance from the hyperplane to the closest data point, which can be expressed as $\frac{2}{\|w\|}$.

\begin{derivation}
Given the hyperplane $\{x \mid w^\top x + w_0 = 0\}$, the signed distance of any point $x$ to the hyperplane is
\[
    \operatorname{dist}(x, \mathcal{H}) = \frac{w^\top x + w_0}{\|w\|}.
\]
Because $y_i \in \{-1, +1\}$, we can enforce the scale of $(w, w_0)$ by requiring that the closest points satisfy $y_i (w^\top x_i + w_0) = 1$. Under this normalization, there exist parallel ``margin'' hyperplanes
\[
    w^\top x + w_0 = 1 \quad \text{and} \quad w^\top x + w_0 = -1
\]
touching the positive and negative classes, respectively. The perpendicular distance between these two planes is the geometric margin:
\[
    \gamma = \frac{1 - (-1)}{\|w\|} = \frac{2}{\|w\|}.
\]
Maximizing $\gamma$ therefore amounts to minimizing $\|w\|$ (or equivalently $\tfrac{1}{2}\|w\|^2$ for convenience) under the constraints $y_i (w^\top x_i + w_0) \ge 1$ for all $i$. This yields the hard-margin SVM formulation
\[
    \min_{w, w_0} \frac{1}{2} \|w\|^2 \quad \text{s.t.}\quad y_i (w^\top x_i + w_0) \ge 1,\; i=1,\ldots,n,
\]
whose solution maximizes the separating margin.
\end{derivation}

\subsection{Slater's Condition}
By assumption that the data is linearly separable, there exists a feasible point $(w, w_0)$ satisfying $y_i (w^\top x_i + w_0) > 1$ for all $i$. This strictly feasible point ensures that Slater's condition holds, guaranteeing strong duality between the primal and dual SVM problems.

\subsection{The Dual}
Writing the hard-margin primal in standard form,
\[
\begin{aligned}
    \min_{w, w_0}\quad & \frac{1}{2} \|w\|^2 \\
    \text{s.t.}\quad & y_i (w^\top x_i + w_0) - 1 \ge 0,\quad i=1,\ldots,n,
\end{aligned}
\]
introduce Lagrange multipliers $\alpha_i \ge 0$ for each margin constraint. The Lagrangian is
\[
    \mathcal{L}(w, w_0, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i \bigl(y_i (w^\top x_i + w_0) - 1\bigr).
\]
To form the dual we minimize $\mathcal{L}$ over the primal variables. Setting derivatives to zero yields the stationarity conditions
\[
    \nabla_w \mathcal{L} = w - \sum_i \alpha_i y_i x_i = 0 \quad \Rightarrow \quad w = \sum_i \alpha_i y_i x_i,
\]
\[
    \frac{\partial \mathcal{L}}{\partial w_0} = -\sum_i \alpha_i y_i = 0.
\]
Substituting back gives the dual objective
\[
    g(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^\top x_j = \sum_{i=1} \alpha _i - \frac{1}{2} || w(\alpha) ||^2,
\]
with constraints $\alpha_i \ge 0$ and $\sum_i \alpha_i y_i = 0$. Hence the dual problem is the quadratic program
\[
\begin{aligned}
    \max_{\alpha \in \mathbb{R}^n}\quad & \sum_{i=1} \alpha _i - \frac{1}{2} || w(\alpha) ||^2 \\
    \text{s.t.}\quad & \alpha_i \ge 0,\quad i=1,\ldots,n, \\
                     & \sum_{i=1}^n \alpha_i y_i = 0.
\end{aligned}
\]
This dual depends only on inner products $x_i^\top x_j$, enabling the kernel trick by replacing them with $k(x_i, x_j)$. The optimal weights follow from the KKT conditions: only training points with $\alpha_i^\star > 0$ (the \emph{support vectors}) contribute to $w^\star = \sum_i \alpha_i^\star y_i x_i$. Complementary slackness enforces $y_i (w^{\star \top} x_i + w_0^\star) = 1$ for active support vectors, which can be used to recover $w_0^\star$ by averaging over any $\alpha_i^\star > 0$.

\begin{notebox}
The dual formulation is preferable when $n$ (number of samples) is smaller than $d$ (feature dimension) or when kernels project data into high-dimensional spaces. It also highlights that margin maximization depends only on a sparse subset of training examples.
\end{notebox}
