\section{Math Preliminaries}
\subsection{Gibbs Distribution}
The Gibbs distribution is a probability distribution that assigns likelihoods to states based on a cost function, with lower-cost states being more probable. Given a set of states $x \in \mathcal{X}$, a cost function $E(x)$, and an inverse temperature parameter $\beta > 0$, the Gibbs distribution is
$$
p(x)=\frac{1}{Z}e^{-\beta E(x)}
$$
where the partition function $Z$ ensures normalization
$$
Z=\sum_{x \in X} e^{-\beta E(x)}
$$

\subsection{Conditional distribution of a multivariate Gaussian}
\label{sec:cond-gaussian}
Let
\[
\begin{bmatrix} a \\ b \end{bmatrix} \sim \mathcal{N}\!\left(\begin{bmatrix} \mu_a \\ \mu_b \end{bmatrix},\
\begin{bmatrix}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{bmatrix}\right),
\]
where $\Sigma_{aa}$ and $\Sigma_{bb}$ are covariance blocks and $\Sigma_{ab} = \Sigma_{ba}^\top$. Then the conditional distribution of $a$ given $b$ is again Gaussian with
\[
\mathbb{E}[a\mid b] = \mu_a + \Sigma_{ab} \, \Sigma_{bb}^{-1} (b - \mu_b),\quad
\operatorname{Cov}(a\mid b) = \Sigma_{aa} - \Sigma_{ab} \, \Sigma_{bb}^{-1} \, \Sigma_{ba}.
\]
This identity underlies Gaussian process prediction, Bayesian linear regression posteriors, and Kalman filtering.

\begin{derivation}
Write the joint as a block Gaussian and use the Schur complement. The joint log-density (up to constants) is
\[
\ell(a,b) = \tfrac{1}{2}\begin{bmatrix} a-\mu_a \\ b-\mu_b \end{bmatrix}^{\!\top}
\begin{bmatrix}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{bmatrix}^{\!-1}
\begin{bmatrix} a-\mu_a \\ b-\mu_b \end{bmatrix}.
\]
Completing the square in $a$ (or applying the standard conditional Gaussian formula) yields
\[
\mathbb{E}[a\mid b] = \mu_a + \Sigma_{ab} \Sigma_{bb}^{-1}(b-\mu_b),\quad
\operatorname{Cov}(a\mid b) = \Sigma_{aa} - \Sigma_{ab} \Sigma_{bb}^{-1} \Sigma_{ba}.
\]
Equivalently, these follow from the block inversion identity and the Schur complement of $\Sigma_{bb}$.
\end{derivation}

\subsection{Schur complement}
\label{sec:schur-complement}
For a block matrix $M = \begin{bmatrix} A & B \\ C & D \end{bmatrix}$ with $D$ invertible, the \textbf{Schur complement of $D$ in $M$} is
\[
S = A - B D^{-1} C.
\]
Key identities (when the required inverses exist):
\begin{itemize}
	\item Determinant: $\det(M) = \det(D)\, \det(S)$.
	\item Block inverse: $M^{-1} = \begin{bmatrix}
	S^{-1} & -S^{-1} B D^{-1} \\
	-D^{-1} C S^{-1} & D^{-1} + D^{-1} C S^{-1} B D^{-1}
	\end{bmatrix}$.
	\item If $M$ is symmetric positive (semi)definite and $D$ is invertible, then $S$ is also positive (semi)definite.
\end{itemize}
Symmetrically, if $A$ is invertible, the Schur complement of $A$ is $D - C A^{-1} B$. In Gaussian conditioning, $\operatorname{Cov}(a\mid b)$ equals the Schur complement of $\Sigma_{bb}$ in the joint covariance.

\subsection{Vector and matrix calculus}
\label{sec:matrix-calculus}
We use the convention that gradients are column vectors. For $x \in \mathbb{R}^d$ and matrices/vectors of compatible dimensions, the following identities are used repeatedly:
\begin{itemize}
    \item $\nabla_x (b^{\top} x) = b$ and $\nabla_x (x^{\top} b) = b$.
    \item $\nabla_x (x^{\top} A x) = (A + A^\top)x$ (and $= 2Ax$ if $A$ is symmetric).
    \item $\nabla_x \|x\|^2_2 = \nabla_x (x^{\top} x) = 2x$.
    \item $\nabla_\beta \|y - X\beta\|^2_2 = -2 X^{\top}(y - X\beta)$.
\end{itemize}

\begin{derivation}
	For $f(x)=b^{\top}x=\sum_i b_i x_i$, we have $\partial f/\partial x_i=b_i$, so $\nabla_x f=b$. The same holds for $x^{\top}b$ since it is the same scalar.
\end{derivation}

\begin{derivation}
To see why $\nabla_x (x^{\top} A x) = (A + A^{\top})x$, expand component-wise:
\[
f(x) = x^{\top} A x = \sum_{i,j} x_i A_{ij} x_j.
\]
Then the $k$-th component of the gradient is
\[
\frac{\partial f}{\partial x_k}
= \sum_j A_{kj} x_j + \sum_i x_i A_{ik}
= (Ax)_k + (A^{\top}x)_k.
\]
Stacking these components yields $\nabla_x f = (A + A^{\top})x$.
\end{derivation}

\begin{derivation}
	For $f(x)=\|x\|^2_2=x^{\top}x$, apply the rule $\nabla_x (x^{\top} A x) = (A + A^\top)x$ with $A = I$. Since $I$ is symmetric, $\nabla_x f = 2Ix = 2x$.
\end{derivation}

\begin{derivation}
For $g(\beta)=\|y-X\beta\|^2=(y-X\beta)^{\top}(y-X\beta)$, expand to $y^{\top}y-2y^{\top}X\beta+\beta^{\top}X^{\top}X\beta$ and apply the previous rules:
\[
\nabla_\beta g = -2X^{\top}y + 2X^{\top}X\beta.
\]
Here we use that $y^{\top}X\beta = (X^{\top}y)^{\top}\beta$, so $\nabla_\beta(-2y^{\top}X\beta)=-2X^{\top}y$. 

Rewriting yields $\nabla_\beta g=-2X^{\top}(y-X\beta)$.
\end{derivation}

