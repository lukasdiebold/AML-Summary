\section{Convex Optimization}

Convex optimization studies problems whose objective and feasible sets are convex, guaranteeing that every local optimum is also global. These structure-driven guarantees let us design algorithms with provable convergence, quantify sensitivity, and provide certificates of optimality via dual variables. This chapter outlines the key definitions, canonical problem classes, duality theory, and foundational algorithms emphasized in the lecture notes.

\subsection{Convex sets, functions, and problems}
Let $C \subseteq \mathbb{R}^d$ be a set. $C$ is \textbf{convex} if for any $x, y \in C$ and $\theta \in [0,1]$, the convex combination $\theta x + (1-\theta) y$ lies in $C$. Typical convex sets are affine subspaces, halfspaces, Euclidean balls, ellipsoids, spectrahedra, and probability simplices.

A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is \textbf{convex} if its domain is convex and for every $x, y$ and $\theta \in [0,1]$
\[
    f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y).
\]
When $f$ is differentiable, convexity is equivalent to the first-order inequality
\[
    f(y) \ge f(x) + \nabla f(x)^\top (y - x),
\]
meaning the affine tangent is a global under-estimator. If $f$ is twice differentiable, convexity is equivalent to $\nabla^2 f(x) \succeq 0$ for all $x$ in the interior of the domain. \textbf{Strong convexity} with parameter $m>0$ strengthens the inequality to
\[
    f(y) \ge f(x) + \nabla f(x)^\top (y-x) + \frac{m}{2}\|y-x\|^2,
\]
implying a unique minimizer and improved convergence rates.

A generic convex optimization problem reads
\[
\begin{aligned}
    \min_{x \in \mathbb{R}^d}\quad & f_0(x) \\
    \text{s.t.}\quad & f_i(x) \le 0,\quad i = 1,\ldots,m, \\
    & Ax = b,
\end{aligned}
\]
where each $f_i$ is convex and the equality constraints define an affine set. The feasible set $\{x \mid f_i(x) \le 0, Ax=b\}$ must be nonempty for the problem to be well-posed. Convexity ensures that any Karush--Kuhn--Tucker (KKT) point is globally optimal.

\subsection{Standard convex programs in machine learning}
Many estimators from earlier chapters fit this template:
\begin{itemize}
    \item \textbf{Least squares / ridge regression:} $f(x)=\tfrac{1}{2}\|Ax - b\|_2^2 + \tfrac{\lambda}{2}\|x\|_2^2$ is convex quadratic; ridge adds strong convexity, yielding the closed form in equation \eqref{eq:map-ridge}.
    \item \textbf{Lasso:} minimize $\tfrac{1}{2}\|Ax-b\|_2^2 + \lambda \|x\|_1$. The $\ell_1$ norm promotes sparsity via a polyhedral penalty while keeping the problem convex. Soft-thresholding is the proximal operator driving coordinate descent.
    \item \textbf{Support Vector Machines (SVM):} hinge-loss objectives $\sum_i \max(0, 1 - y_i w^\top x_i)$ with $\ell_2$ regularization define convex problems whose dual has sparse support vectors.
    \item \textbf{Logistic regression:} the negative log-likelihood $\sum_i \log(1 + \exp(-y_i w^\top x_i))$ is convex. Adding $\ell_1$ or $\ell_2$ penalties yields generalized linear models solvable via gradient or Newton methods.
    \item \textbf{Matrix completion:} minimizing $\sum_{(i,j)\in\Omega}(X_{ij} - M_{ij})^2 + \lambda \|X\|_\ast$ uses the nuclear norm (convex surrogate of rank) to recover low-rank matrices.
\end{itemize}

Specialized subfamilies often admit faster algorithms:
\begin{itemize}
    \item \textbf{Linear programs (LP):} $f_0(x) = c^\top x$, $f_i(x) = a_i^\top x - b_i$.
    \item \textbf{Quadratic programs (QP):} quadratic objective with linear constraints.
    \item \textbf{Second-order cone programs (SOCP)} and \textbf{semidefinite programs (SDP)} capture norms and PSD constraints, respectively, and power robust control plus covariance fitting.
\end{itemize}

\subsection{Lagrangian duality and KKT conditions}
For
\[
\min_x f_0(x) \quad \text{s.t.}\quad f_i(x) \le 0,\, Ax = b,
\]
introduce multiplier $\lambda \ge 0$ for inequalities and $\nu$ for equalities. The \textbf{Lagrangian}
\[
    \mathcal{L}(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \nu^\top (Ax - b)
\]
lower-bounds the primal objective: $g(\lambda, \nu) = \inf_x \mathcal{L}(x, \lambda, \nu) \le f_0(x)$ for all feasible $x$. The \textbf{dual problem} maximizes $g(\lambda,\nu)$ subject to $\lambda \ge 0$. Weak duality ($g \le p^\star$) always holds, while \textbf{strong duality} ($g^\star = p^\star$) holds under mild constraint qualifications such as Slater's condition (strict feasibility). Dual variables often have sensitivity interpretations, e.g., the effect of tightening constraints.

The \textbf{KKT conditions} describe optimality when strong duality holds:
\begin{align*}
    &\text{Primal feasibility: } f_i(x^\star) \le 0,\; Ax^\star = b. \\
    &\text{Dual feasibility: } \lambda^\star \ge 0. \\
    &\text{Stationarity: } \nabla f_0(x^\star) + \sum_i \lambda_i^\star \nabla f_i(x^\star) + A^\top \nu^\star = 0. \\
    &\text{Complementary slackness: } \lambda_i^\star f_i(x^\star) = 0.
\end{align*}
For unconstrained problems these reduce to $\nabla f_0(x^\star) = 0$, consistent with calculus. In SVMs, for example, KKT implies that only points on the margin have non-zero dual variables, explaining sparsity in the dual solution.

\subsection{First-order algorithms}
Most large-scale machine learning problems rely on first-order (gradient-based) methods with low per-iteration cost.
\begin{itemize}
    \item \textbf{Gradient descent (GD).} Iterate $x_{k+1} = x_k - \eta_k \nabla f(x_k)$. For $L$-smooth convex $f$, constant step-size $\eta = 1/L$ yields $f(x_k) - f^\star \le \tfrac{L\|x_0 - x^\star\|^2}{2k}$. If $f$ is additionally $m$-strongly convex, the error contracts linearly: $f(x_k) - f^\star \le \left(1 - \tfrac{m}{L}\right)^k (f(x_0) - f^\star)$.
    \item \textbf{Projected gradient.} For constrained problems with easy projections $\Pi_C$, update $x_{k+1} = \Pi_C(x_k - \eta \nabla f(x_k))$. This applies to simplex constraints, $\ell_2$-balls, or PSD cones (via eigenvalue thresholding).
    \item \textbf{Subgradient methods.} When $f$ is convex but nonsmooth (e.g., hinge or $\ell_1$ norms), use $x_{k+1} = x_k - \eta g_k$ with $g_k \in \partial f(x_k)$. Convergence is $O(1/\sqrt{k})$ for general convex functions.
    \item \textbf{Accelerated methods.} Nesterov's accelerated gradient adds a momentum term achieving $f(x_k) - f^\star = O(1/k^2)$ for $L$-smooth convex objectives, approaching optimal complexity for first-order methods.
    \item \textbf{Stochastic gradients.} SGD and its variants (momentum, Adam, Adagrad) replace $\nabla f$ with unbiased estimates, giving scalable training on massive datasets. Convexity enables convergence proofs, e.g., $\mathbb{E}[f(\bar{x}_k)] - f^\star = O(1/\sqrt{k})$ with diminishing steps.
\end{itemize}

\subsection{Second-order and interior-point methods}
Second-order methods leverage curvature for faster local convergence.
\begin{itemize}
    \item \textbf{Newton's method.} Update $x_{k+1} = x_k - \nabla^2 f(x_k)^{-1} \nabla f(x_k)$. Under mild conditions, Newton steps enjoy quadratic convergence near $x^\star$, but each iteration requires solving a linear system (often via Cholesky or conjugate gradients). Damped Newton with line search ensures global convergence for convex $f$.
    \item \textbf{Quasi-Newton methods.} BFGS or L-BFGS approximate Hessians by low-rank updates, offering superlinear convergence without storing the full Hessian. Widely used for logistic regression and other smooth convex losses.
    \item \textbf{Interior-point methods (IPM).} For constrained problems, IPMs solve a sequence of barrier subproblems
    \[
        \phi_\mu(x) = f_0(x) - \mu \sum_{i=1}^m \log(-f_i(x))
    \]
    using Newton steps. As $\mu \rightarrow 0$, solutions trace the \textit{central path} toward the primal optimum while satisfying constraints. IPMs bring polynomial-time worst-case guarantees for LPs, QPs, SOCPs, and SDPs, albeit with higher per-iteration complexity than first-order methods.
\end{itemize}

\subsection{Proximal, splitting, and decomposition techniques}
When $f = g + h$ with $g$ smooth and $h$ simple but possibly nonsmooth, \textbf{proximal methods} exploit the proximal operator
\[
    \operatorname{prox}_{\eta h}(v) = \arg\min_x \left\{ h(x) + \frac{1}{2\eta}\|x - v\|^2 \right\}.
\]
Common examples include soft-thresholding (for $\ell_1$ norms) or projection onto convex sets. Algorithms include:
\begin{itemize}
    \item \textbf{Proximal gradient / ISTA}: $x_{k+1} = \operatorname{prox}_{\eta h}(x_k - \eta \nabla g(x_k))$. FISTA accelerates this to $O(1/k^2)$ convergence.
    \item \textbf{Alternating direction method of multipliers (ADMM)}: solves $\min_x g(x) + h(z)$ subject to $Mx + Nz = c$ by alternating minimization with dual updates. ADMM decomposes problems across blocks, enabling distributed or parallel implementations (e.g., consensus optimization).
    \item \textbf{Augmented Lagrangian methods}: strengthen dual ascent by penalizing constraint violation quadratically, improving stability compared to vanilla dual methods.
\end{itemize}

\subsection{Sensitivity, certificates, and modeling tips}
\begin{itemize}
    \item \textbf{Dual variables as sensitivities.} At optimum, $\lambda_i^\star$ equals the marginal increase in the optimal value if constraint $i$ is tightened. Inspecting $\lambda^\star$ highlights bottleneck constraints or active data points.
    \item \textbf{Certificates of optimality.} A feasible primal-dual pair satisfying KKT provides a certificate. Numerically, interior-point solvers report the primal-dual gap $\|f(x) - g(\lambda,\nu)\|$, which bounds suboptimality.
    \item \textbf{Scaling.} Normalize features and constraints so that Hessians and gradients are well-conditioned; this accelerates both first- and second-order methods.
    \item \textbf{Modeling systems.} Packages like CVX, CVXPY, or disciplined convex programming (DCP) frameworks enforce convexity by construction, automatically converting high-level specifications into standard cone programs.
    \item \textbf{Stopping criteria.} Monitor residuals, objective gaps, and constraint violation simultaneously; mere gradient norm may be insufficient when constraints are present.
\end{itemize}

Convex optimization thus supplies the theoretical backbone for many machine learning estimators and algorithms: it furnishes tractable objectives, certifiable solutions, and scalable methods tailored to problem structure. Understanding duality and algorithmic trade-offs lets practitioners choose the right solver for each model in the AML toolbox.
