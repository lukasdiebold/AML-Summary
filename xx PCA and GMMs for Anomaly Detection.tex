\section{PCA and GMMs for Anomaly Detection}
The task of anomaly detection involves identifying data points that deviate significantly from the norm. In situations like banks needing to detect fraudulent transactions, or hospitals aiming to identify unusual patient health metrics, effective anomaly detection methods are crucial.
\subsection{Anomalies}
Typically, we have a space of all possible events $\mathcal X$ and a subset called the normal set $\mathcal N \subseteq \mathcal X$. An anomaly is any event $x \in \mathcal X$ such that $x \notin \mathcal N$. What makes this challenging is that the normal set $\mathcal N$ is often not explicitly defined, and we may only have access to a limited set of examples from $\mathcal N$. The way we approach this problem is to do a dimensionality reduction $\Pi$ on the data to find a lower-dimensional representation that captures the essential structure of the normal data. We can then use this representation to identify anomalies.

The generalized approach looks like this:
\begin{enumerate}
    \item An anomaly is an unlikely event.
    \item We fit a model of a parametric family of distributions $\mathcal H = \{p(x; \theta) : \theta \in \Theta\}$
    \item We define an anomaly score $-\log p_{\hat{\theta}}(x)$
\end{enumerate}

We choose a GMM (Gaussian Mixture Model) as our parametric family of distributions. A GMM is a weighted sum of multiple Gaussian distributions, which allows us to model complex data distributions effectively. The parameters $\theta$ of the GMM include the means, covariances, and mixture weights of the individual Gaussian components. This is a good choice, because it has been observed that linear projections of high-dimensional distributions onto low-dimensional spaces resemble Gaussian distributions, thie can partially be explained by the Central Limit Theorem.

This leads us to the following algorithm for anomaly detection using PCA and GMMs:
\begin{enumerate}
    \item Project the data linearly onto a lower-dimensional space (using PCA)
    \item Fit a GMM to the projected data 
\end{enumerate}

\subsection{Dimensionality Reduction}
Given $X=\left\{x_1, \ldots, x_n\right\} \subseteq \mathbb{R}^D$, find a linear projection $\pi: \mathbb{R}^D \rightarrow \mathbb{R}^d$, with $d \ll D$, and such that $\pi(X)$ has a "sufficiently large" variance. So we want a projection $\pi$ that maximizes the variance of the projected data points.

\subsubsection{First Stage}
We start with a very tractable case which might be a bit too simple. We look for a one-dimensional projection $\pi(x) = w^T x$, where $w \in \mathbb{R}^D$ is a unit vector (i.e., $\|w\|_2 = 1$). The variance of the projected data points is given by:
\begin{equation}
    \text{Var}(\pi(X)) = \frac{1}{n} \sum_{i=1}^n (w^T x_i - w^T \mu)^2 = w^T \Sigma w
\end{equation}
where $\mu = \frac{1}{n} \sum_{i=1}^n x_i$ is the mean of the data points, and $\Sigma = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)(x_i - \mu)^T$ is the covariance matrix of the data points.
To find the optimal projection vector $w$, we need to solve the following optimization problem:
\begin{equation}
    \max_{w \in \mathbb{R}^D} w^T \Sigma w \quad \text{subject to} \quad \|w\|_2 = 1
\end{equation}
Where the solution is given by the eigenvector of $\Sigma$ corresponding to the largest eigenvalue.

\subsubsection{General Case}
To generalize this to a $d$-dimensional projection, we can define the projection as $\pi(x) = W^T x$, where $W \in \mathbb{R}^{D \times d}$ is a matrix with orthonormal columns (i.e., $W^T W = I_d$). We proceed similarly to the one-dimensional case, and we want to maximize the variance of the projected data points:
\begin{equation}
    \text{Var}(\pi(X)) = \frac{1}{n} \sum_{i=1}^n \|W^T x_i - W^T \mu\|_2^2 = \text{tr}(W^T \Sigma W)
\end{equation}
To find the optimal projection matrix $W$, we need to solve the following optimization problem:
\begin{equation}
    \max_{W \in \mathbb{R}^{D \times d}} \text{tr}(W^T \Sigma W) \quad \text{subject to} \quad W^T W = I_d
\end{equation}
The solution is given by the matrix $W$ whose columns are the eigenvectors of $\Sigma$ corresponding to the $d$ largest eigenvalues.

\subsection{Fitting a GMM}
\subsubsection{GMM Definition}
A Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of several Gaussian distributions. The probability density function of a GMM with $K$ components is given by:
\begin{equation}
    p(x; \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(x; \mu_k, \Sigma_k)
\end{equation}
where $\pi_k$ are the mixture weights (with $\sum_{k=1}^K \pi_k = 1$ and $\pi_k \geq 0$), $\mu_k$ are the means, and $\Sigma_k$ are the covariance matrices of the individual Gaussian components. The parameters of the GMM are denoted by $\theta = \{\pi_k, \mu_k, \Sigma_k\}_{k=1}^K$.

\subsection{Fitting}
We again use a MLE approach, that is, we want to find the parameters $\hat{\theta}$ that maximize the likelihood of the observed data:
\begin{align}
    \log p_\theta(x) &= \log \prod_{i=1}^n p(x_i; \theta) \\
    &= \sum_{i=1}^n \log p(x_i; \theta) \\
     &= \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k) \right)
\end{align}

However, this last term is difficult to optimize directly due to the presence of the logarithm of a sum. We make an interesting observation: if we knew which component generated each data point, the optimization would be much simpler. This leads us to introduce latent variables $z_i$ that indicate the component responsible for generating each data point $x_i$. Specifically, we define $z_i$ as a one-hot encoded vector where $z_{ik} = 1$ if the $i$-th data point was generated by the $k$-th component, and $0$ otherwise. With these latent variables, we can express the complete-data log-likelihood as:
\begin{align}
\log p_\theta(X, Z) & =\log p_\theta(Z) p_\theta(X \mid Z) \\
& =\log p_\theta(Z)+\log p_\theta(X \mid Z) \\
& =\sum_{i \leq n} \log \pi_{Z_i}+\sum_{i \leq n} \log p_\theta\left(X_i \mid Z_i\right)
\end{align}
Where $Z = \{z_1, \ldots, z_n\}$ is the set of latent variables for all data points. This is way easier to optimize.

Now we have the setup, that we have $\log p_\theta(x)$ but we would like to work with $\log p_\theta(X, Z)$. We know that:
\begin{align}
     & \log p_\theta(x) = \log \frac{p_\theta(x, z)}{p_\theta(z | x)} \\
    \Rightarrow & \mathbb E_{Z \sim q}[\log p_\theta(x)] = \mathbb E_{Z \sim q}\left[\log \frac{p_\theta(x, Z)}{p_\theta(Z | x)}\right] \\
\end{align}
Since the left-hand side does not depend on $Z$, we can write:
\begin{align}
    \log p_\theta(x) & = \mathbb E_{Z \sim q}\left[\log \frac{p_\theta(x, Z)}{p_\theta(Z | x)}\right] \\
    & = \mathbb E_{Z \sim q}\left[\log \frac{p_\theta(x, Z)}{p_\theta(Z | x)} \cdot \frac{q(Z)}{q(Z)}\right] \\
    & = \mathbb E_{Z \sim q}[\log p_\theta(x, Z)-\log q(Z)] + \mathbb E_{Z \sim q}\left[\log \frac{q(Z)}{p_\theta(Z | x)}\right] \\
    & = M(q, \theta) + \text{KL}(q \| p_\theta(Z | x))
\end{align}
Where we defined:
\begin{equation}
    M(q, \theta) = \mathbb E_{Z \sim q}[\log p_\theta(x, Z)-\log q(Z)]
\end{equation}
This gives us a lower bound on the log-likelihood, since the KL divergence is always non-negative:
\begin{equation}
    \log p_\theta(x) \geq M(q, \theta)
\end{equation}
and we have equality if and only if $q(Z) = p_\theta(Z | x)$.

\subsection{EM Algorithm}
The Expectation-Maximization (EM) algorithm is an iterative method used to find maximum likelihood estimates of parameters in statistical models with latent variables. In the context of fitting a GMM, the EM algorithm alternates between two main steps: the Expectation (E) step and the Maximization (M) step.
\begin{itemize}
    \item \textbf{E-step:} In this step, we compute the expected value of the complete-data log-likelihood with respect to the current estimate of the parameters $\theta^{(t)}$. This involves calculating the posterior probabilities of the latent variables given the observed data and the current parameter estimates. Specifically, we set:
    \begin{equation}
        q^{(t+1)}(Z) = p_{\theta^{(t)}}(Z | X)
    \end{equation}
    \item \textbf{M-step:} In this step, we maximize the expected complete-data log-likelihood computed in the E-step with respect to the parameters $\theta$. This gives us updated parameter estimates:
    \begin{equation}
        \theta^{(t+1)} = \arg\max_\theta M(q^{(t+1)}, \theta)
    \end{equation}
\end{itemize}
The EM algorithm iterates between these two steps until convergence, which is typically determined by checking if the change in the log-likelihood or the parameter estimates falls below a predefined threshold.

\subsection{Validation Metrics}
We want a function $\phi : \mathbb{R}^D \rightarrow \{0, 1\}$ that that classifies points as normal (0) or anomalous (1). Given a threshold $\tau$. We have two objectives: (let $C = \{x : \phi(x) = 1\}$ be the set of points classified as anomalous, and $A$ be the set of true anomalies)
\begin{itemize}
    \item If $x \in A$, then $\phi(x) = 1$
    \begin{equation}
        \text{Recall} = \frac{|C \cap A|}{|A|}
    \end{equation}
    'Reatio of the correctly identified anomalous among \textbf{all truly anomalous} points'

    How many of the actual anomalous points did we identify?
    \item If $\phi(x) = 1$, then $x \in A$ (Precision)
    \begin{equation}
        \text{Precision} = \frac{|C \cap A|}{|C|}
    \end{equation}
    'Ratio of the correctly identified anomalous points among \textbf{all identified anomalous points}'

    How many of the points we classified as anomalous are actually anomalous?
\end{itemize}
We can combine these two metrics into a single metric called the F1-score, which is the harmonic mean of precision and recall:
\begin{equation}
    \text {F1 Score }=\frac{2}{\frac{1}{\text { Precision }}+\frac{1}{\text { Recall }}}
\end{equation}
The F1-score provides a balanced measure of the model's performance in identifying anomalies, taking into account both precision and recall.

\subsection{Combined Pipeline}
Given a set $X \in \mathbb{R}^D$ of "normal" points, we train an anomaly detector as follows.
\begin{enumerate}
    \item We compute a projector $\pi: \mathbb{R}^D \rightarrow \mathbb{R}^d$ using PCA.
    \item We then fit a pdf $p_\theta(\cdot)$ with k -components to $\{\pi(x): x \in X\}$, using the EM algorithm.
    \item For a new point, its "anomaly score" is $-\log p_\theta(\pi(x))$.
    
\end{enumerate}