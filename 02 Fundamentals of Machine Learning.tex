\section{Fundamentals of Machine Learning}

Machine learning is fundamentally about inferring models from data. At its core lies Bayes' rule, which relates the posterior distribution (model given data) to the likelihood and prior:
$$
\mathbb{P}( \text{model} \mid \text{data}) =\frac{\mathbb{P}(\text { data } \mid \text { model }) \mathbb{P}(\text { model })}{\mathbb{P}(\text { data })}
$$

In the \textbf{maximum likelihood (ML)} approach, we select the model that maximizes the likelihood of observing the data:
$$
\widehat{\text { model }} \in \arg \max _{\text {model }} \mathbb{P} (\text{data} \mid \text{model})
$$
Under regularity conditions, the ML estimator $\widehat{\text{model}_n}$ is consistent, asymptotically normal, and asymptotically efficient.

\textbf{Consistency:} A point estimator $\hat{\theta}_{n}$ of the parameter $\theta=\theta_{0}$ is consistent if it converges in probability to the true parameter:
$$
\forall \varepsilon>0,  \mathbb{P}\left(\left|\hat{\theta}_{n}-\theta_{0}\right|>\varepsilon\right) \xrightarrow{n \rightarrow \infty} 0
$$

More formally, using the $\varepsilon$-$\delta$ definition:
$$\forall \theta, \forall \varepsilon, \delta>0, \exists n_{0}, \forall n>n_{0}, \mathbb{P}\left(\left|\widehat{\theta}_{n}-\theta\right|<\varepsilon\right)>1-\delta
$$

\textbf{Efficiency:} An estimator $\hat{\theta}_n$ is efficient if it achieves the minimum mean squared error among all estimators:
$$
\hat{\theta}_{n}=\arg \min_{\hat{\theta}} \mathbb{E}\left[\left(\hat{\theta}_{n}-\theta_{0}\right)^{2}\right]
$$
The question then arises: how precisely can we estimate $\theta$ given $n$ samples? The Cramér-Rao bound provides a fundamental lower bound on the variance of any unbiased estimator.

\subsection{Efficiency: Cramér-Rao Bound}

\textbf{Problem:} What is the best achievable precision for parameter estimation?

Given a likelihood $p(y \mid \theta)$ for $\theta \in \Theta$ and data $y_{1}, \ldots, y_{n} \sim p\left(y \mid \theta=\theta_{0}\right)$, we ask: How precisely can we estimate $\theta=\theta_{0}$ given $n$ samples?

For an estimator $\hat{\theta}\left(y_{1}, \ldots, y_{n}\right)$, we measure precision via the mean squared error:
$$\mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\theta)^{2}\right]$$

\textbf{Key definitions:}
\begin{itemize}
    \item Score: $\Lambda=\frac{\partial}{\partial \theta} \log p(y \mid \theta)=\frac{\frac{\partial}{\partial \theta} p(y \mid \theta)}{p(y \mid \theta)}$
    \item Bias: $b_{\hat{\theta}}=\mathbb{E}_{y \mid \theta}\left[\hat{\theta}\left(y_{1}, \ldots, y_{n}\right)\right]-\theta$
\end{itemize}

\textbf{Expected score:} The score has zero mean.
$$
\begin{aligned}
\mathbb{E}_{y \mid \theta}[\Lambda] & =\int p(y \mid \theta) \frac{\frac{\partial}{\partial \theta} p(y \mid \theta)}{p(y \mid \theta)} d y \\
& =\frac{\partial}{\partial \theta} \int p(y \mid \theta) d y=\frac{\partial}{\partial \theta} 1=0
\end{aligned}
$$

\textbf{Score-estimator product:}
$$
\begin{aligned}
\mathbb{E}_{y \mid \theta}[\Lambda \hat{\theta}] & =\int p(y \mid \theta) \frac{\frac{\partial}{\partial \theta} p(y \mid \theta)}{p(y \mid \theta)} \hat{\theta} d y \\
& =\frac{\partial}{\partial \theta}\left(\int p(y \mid \theta) \hat{\theta} d y\right) \\
& =\frac{\partial}{\partial \theta}\left(\mathbb{E}_{y \mid \theta} \hat{\theta}\right)=\frac{\partial}{\partial \theta}\left(b_{\hat{\theta}}+\theta\right)=\frac{\partial}{\partial \theta} b_{\hat{\theta}}+1
\end{aligned}
$$

\textbf{Cross-correlation:}
$$
\mathbb{E}_{y \mid \theta}\left[\left(\Lambda-\mathbb{E} \Lambda\right)\left(\hat{\theta}-\mathbb{E} \hat{\theta}\right)\right]=\mathbb{E}_{y \mid \theta}[\Lambda \hat{\theta}]-\mathbb{E}_{y \mid \theta}[\Lambda] \mathbb{E} \hat{\theta}=\mathbb{E}_{y \mid \theta}[\Lambda \hat{\theta}]
$$
since $\mathbb{E}[\Lambda]=0$.

\textbf{Cauchy-Schwarz inequality:} Applying Cauchy-Schwarz to the cross-correlation:
$$
\begin{aligned}
\left(\mathbb{E}_{y \mid \theta}[\Lambda(\hat{\theta}-\mathbb{E} \hat{\theta})]\right)^{2} & \leq \mathbb{E}_{y \mid \theta}\left[\Lambda^{2}\right] \mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\mathbb{E} \hat{\theta})^{2}\right]
\end{aligned}
$$
Expanding the right-hand side:
$$
\begin{aligned}
\mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\mathbb{E} \hat{\theta})^{2}\right] &= \mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\theta+\theta-\mathbb{E} \hat{\theta})^{2}\right] \\
&=\mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\theta)^{2}\right]-b_{\hat{\theta}}^{2}
\end{aligned}
$$
Therefore:
$$
\left(\frac{\partial}{\partial \theta} b_{\hat{\theta}}+1\right)^{2} \leq \mathbb{E}_{y \mid \theta}\left[\Lambda^{2}\right]\left(\mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\theta)^{2}\right]-b_{\hat{\theta}}^{2}\right)
$$
Rearranging yields the \textbf{Cramér-Rao bound}:
$$
\mathbb{E}_{y \mid \theta}\left[(\hat{\theta}-\theta)^{2}\right] \geq \frac{\left(\frac{\partial}{\partial \theta} b_{\hat{\theta}}+1\right)^{2}}{\mathbb{E}_{y \mid \theta}\left[\Lambda^{2}\right]}+b_{\hat{\theta}}^{2}
$$

\textbf{Fisher information:} The expected squared score is called the Fisher information:
$$
I(\theta) := \mathbb{E}_{y \mid \theta}\left[\Lambda^{2}\right]=\int p(y \mid \theta)\left(\frac{\partial}{\partial \theta} \log p(y \mid \theta)\right)^{2} d y
$$
It measures how much information the data contains about the parameter $\theta$. Higher Fisher information means we can estimate $\theta$ more precisely.

\textbf{Remarks:}
\begin{itemize}
    \item For unbiased estimators ($b_{\hat{\theta}}=0$), the bound simplifies to $\mathbb{E}[(\hat{\theta}-\theta)^2] \geq 1/I(\theta)$.
    \item The bound reveals a trade-off for biased estimators: reducing bias derivative $\frac{\partial}{\partial \theta} b_{\hat{\theta}}$ vs. reducing squared bias $b_{\hat{\theta}}^{2}$. Unbiased estimators are not always optimal!
\end{itemize}

\textbf{Fisher information for $n$ i.i.d. samples:}
$$
\begin{aligned}
I^{(n)}(\theta) &= \mathbb{E}_{y_{1}, \ldots, y_{n} \mid \theta}\left[\Lambda^{2}\right] \\
&= \mathbb{E}\left[\left(\frac{\partial}{\partial \theta} \log p\left(y_{1}, \ldots, y_{n} \mid \theta\right)\right)^2\right] \\
&= \mathbb{E}\left[\left(\sum_{i=1}^n \frac{\partial}{\partial \theta} \log p\left(y_{i} \mid \theta\right)\right)^{2}\right] = \mathbb{E}\left[\left(\sum_{i=1}^n \Lambda_{i}\right)^{2}\right] \\
&= \sum_{i=1}^n \mathbb{E}\left[\Lambda_{i}^{2}\right]+\sum_{i \neq j} \mathbb{E}\left[\Lambda_{i}\right] \mathbb{E}\left[\Lambda_{j}\right] \\
&= \sum_{i=1}^n \mathbb{E}\left[\Lambda_{i}^{2}\right] = n I(\theta)
\end{aligned}
$$
where the cross-terms vanish because $\mathbb{E}[\Lambda_i]=0$ and the samples are independent.

\textbf{Key insight:} The Fisher information of $n$ i.i.d. random variables is $n$ times the Fisher information of a single random variable. This shows that precision improves linearly with sample size.



