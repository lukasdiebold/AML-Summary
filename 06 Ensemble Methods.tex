\section{Ensemble Methods}

Ensemble methods combine multiple base learners to obtain a predictor whose bias, variance, or loss surface is superior to any constituent model. By averaging or sequentially correcting learners trained on diverse views of the data, ensembles produce more stable predictions, calibrated uncertainties, and richer inductive biases than single models. This chapter summarizes the bootstrap-based family (bagging and random forests), boosting, and more general stacking strategies, with an emphasis on how they target different points along the bias--variance trade-off.

\subsection{Why ensembles help}
Let $\hat f(x)$ be a single hypothesis trained on data $D$. Ensembles form $\hat f_{\text{ens}}(x) = \sum_{b=1}^B w_b \hat f_b(x)$ from base learners $\hat f_b$. Two canonical effects explain their success:
\begin{itemize}
    \item \textbf{Variance reduction.} If the $\hat f_b$'s are identically distributed with variance $\sigma^2$ and pairwise correlation $\rho$, then
    \[
        \operatorname{Var}\bigl[\hat f_{\text{avg}}(x)\bigr] = \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2,
    \]
    which shrinks as $B$ increases whenever $\rho < 1$. Bagging manipulates the data (bootstrap samples) to drive correlations down and thus stabilize high-variance learners such as decision trees or neural networks trained on small data.
    \begin{derivation}
    For the average predictor $\hat f_{\text{avg}}(x) = \tfrac{1}{B} \sum_{b=1}^B \hat f_b(x)$,
    \[
        \begin{aligned}
        \operatorname{Var}[\hat{f}(x)] & =\mathbb{E}_D\left(\hat{f}(X)-\mathbb{E}_D \hat{f}(X)\right)^2 \\
        & =\mathbb{E}_D\left(\frac{1}{B} \sum_{i=1}^B \hat{f}_i(x)-\frac{1}{B} \sum_{i=1}^B \mathbb{E}_D \hat{f}_i(x)\right)^2 \\
        & =\mathbb{E}_D\left(\frac{1}{B} \sum_{i=1}^B\left(\hat{f}_i(x)-\mathbb{E}_D \hat{f}_i(x)\right)\right)^2 \\
        & =\frac{1}{B^2} \sum_{i=1}^B \operatorname{Var}_D\left[\hat{f}_i(x)\right]+\frac{1}{B^2} \sum_{i \neq j} \operatorname{Cov}\left[\hat{f}_i(x), \hat{f}_j(x)\right]
        \end{aligned}
    \]
    If all base learners share variance $\sigma^2$ and pairwise covariance $\operatorname{Cov}(\hat f_i, \hat f_j) = \rho\,\sigma^2$,
    \begin{align*}
    \operatorname{Var}\bigl[\hat f_{\text{avg}}(x)\bigr]
    &= \frac{B}{B^2} \sigma^2 + \frac{B(B-1)}{B^2} \, \rho\,\sigma^2 \\
    &= \frac{1}{B}\,\sigma^2 + \left(1 - \frac{1}{B}\right) \rho\,\sigma^2 \\
    &= \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2.
    \end{align*}
    Thus variance decreases like $1/B$ when correlations are small ($\rho \approx 0$), while residual correlation limits the gain.
    \end{derivation}
    \item \textbf{Bias correction.} Sequential ensembles such as boosting fit a series of weak learners to the residuals (negative gradients) of the current model. Each stage nudges the predictor toward lower bias and can transform a barely better-than-random base learner into a strong classifier.
\end{itemize}
These mechanisms are complementary: bagging primarily attacks variance, boosting primarily attacks bias, and stacking blends heterogeneous models to capture complementary inductive biases.

\subsection{Boosting}
For now we're only interested in classification.

Given a Dataset $D = \{(x_i, y_i)\}_{i=1} \subseteq \mathbb R^d \times \{-1, +1\} \sim p_*$, where $y_i$ are labels and $x_i$ are features, with an unknown distribution $p_*$, we want to learn a classifier $G: \mathbb R^d \to \{-1, +1\}$ that minimizes the expected 0--1 loss:
\[L(G) = \mathbb E_{(X, Y) \sim p_*}[\mathbb I \{G(X) \neq Y\}]\]
there are three problems with this formulation:
\begin{itemize}
    \item The distribution $p_*$ is unknown, so we cannot compute the expected loss directly.
    \item The 0--1 loss is not differentiable, making optimization difficult.
    \item The hypothesis space of all classifiers $G: \mathbb R^d \to \{-1, +1\}$ is too large to search over.
\end{itemize}
To address these issues, we make the following changes:
\begin{itemize}
    \item We replace the expected loss with the empirical loss on the training data:
    \[\hat L(G, D) = \frac{1}{n} \sum_{i=1}^n \mathbb I \{G(x_i) \neq y_i\}\]
    \item We replace the 0--1 loss with a surrogate loss function $\ell: \mathbb R \to \mathbb R_+$ that is differentiable and convex, such as the exponential loss $\ell(z) = e^{-yG(x)}$.
    \item We restrict our hypothesis space to a set of weak learners $\mathcal H^A$ ($A$ for additive), with
    \[
        \mathcal H^A = \left\{G_m | G_m(x) = \sum_{i\leq m} \beta_i f_i(x), f_i \in \mathcal H, \beta_i \in \mathbb R\right\}
    \]
\end{itemize}
This won't work with linear models. But with weak learners (e.g., decision stumps), we can use boosting to iteratively build a strong classifier by adding weak learners that minimize the surrogate loss on the training data.

So then our objective becomes:
\[
\min_{G \in \mathcal H^A} \frac{1}{n} \sum_{i\leq n}\mathcal L(y_i; G(x_i))
\]
with 
\[
    \mathcal L(y_i; G(x)) = \exp(-y_i G(x_i))
    = \exp(-y_i G_{m-1}(x_i))\exp(-y\beta_mf_m(x_i))
    = w_i^{m-1} \mathcal L(y_i; \beta_m f_m(x_i))
\]
because $G_m(x)=G_{m-1}(x)+\beta_m f_m(x)$. What did we do? We decomposed the loss at step $m$ into the loss at step $m-1$ and the contribution of the new weak learner $f_m$ with weight $\beta_m$. So now we have two things to optimize: the weak learner $f_m$ and its weight $\beta_m$. 

and we can rewrite the objective as:
\[
\min_{G_{m-1} \in \mathcal H^A_{m-1}} \min_{f_m \in \mathcal H, \beta_m \in \mathbb R} \sum_{i\leq n} w_i^{m-1} \mathcal L(y_i; \beta_m f_m(x_i))
\]

\subsection{Forward Stagewise Additive Modeling (FSAM)} 
is a greedy approach to solve this optimization problem. At each step $m$, we fix the previous model $G_{m-1}$ and optimize for the new weak learner $f_m$ and its weight $\beta_m$:

\begin{algorithmic}
\State $G_0(x) \gets 0 $, $w_i^0 \gets 1/n$ for all $i$
\If{$t = 1$ to $m$} 
    \State $\min_{f_m \in \mathcal H, \beta_m \in \mathbb R} \frac{1}{n} \sum_{i\leq n} w_i^{t-1} \mathcal L(y_i; \beta_t f_t(x_i))$
    \State $w_i^t \gets w_i^{m-1} \exp(-y_i G_{t-i}(x_i))$
\EndIf 
\end{algorithmic}
Future classifieres try to correct the mistakes of past classifiers by focusing more on misclassified examples (increasing their weights). This way, the ensemble learns from its errors and improves over time.

\subsection{Gradient Descent Boosting}
Wantde $G_{\theta}(x)$ where $\theta \in \Theta$ are parameters. We can use gradient descent to minimize the empirical loss:

    \begin{algorithmic}
    \State $\theta_0 = \$$
\For $t=1$ to $T$
    \State $\theta_t \leftarrow \theta_{t-1}-\alpha_t \nabla_\theta \mathcal{L}\left(D ; \theta_{t-1}\right)$
\EndFor
\end{algorithmic}
At then end
\[
\theta_T = \sum_{t\leq T} \alpha _t \nabla_\theta \mathcal L(D; \theta_{t-1})
\]
The combined algorithm looks like this:
\begin{algorithmic}
\State $G_0(x) \gets 0$
\For{$t = 1$ to $m$}
    \State $r_i^{t-1} \gets -\left[\frac{\partial \mathcal L(y_i; G(x_i))}{\partial G(x_i)}\right]_{G(x)=G_{t-1}(x)}$ for all $i$
    \State $\min_{f_t \in \mathcal H, \beta_t \in \mathbb R} \sum_{i\leq n} (r_i^{t-1} - \beta_t f_t(x_i))^2$
    \State $G_t(x) \gets G_{t-1}(x) + \beta_t f_t(x)$
\EndFor
\end{algorithmic}