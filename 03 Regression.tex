\section{Regression}
\subsection{Act 1: High Dimensional regression is unstable}

we assume X and y are distributed according to a distribution $p_*$ (i.e. $X, y \sim p_*$) and 
$$
y = f_*(x) + \varepsilon \quad \text{with } \varepsilon \sim \mathcal N(0, \sigma)
$$
Then our task ist to estimage $f_*$ from $D = \{x_i, y_i\} \sim p_*$. The problem in this form isn't tractable, this is why we restrict the choice of functions. First we choose the limit ourselfs to linear functions, that is function of the form 
$$f_*(x) = \beta^\top x$$
This problem we then solve using Maximum Likelihood Estimation (MLE), that is
$$
\begin{aligned}
\widehat{\beta} &=\arg \max _{\beta \in \mathbb{R}^R} p(D \mid \beta)\\
&= ... \\
&= \arg \min \text{MSE}(D, \beta) \\
&= \frac{1}{n} \sum_{i \leq n} (y_i - \beta ^\top x_i)^2\\
&= (X^\top X)^{-1} X^\top y \\
&= ...\\
&= X^\top (X X ^\top )^{-1}y
\end{aligned}
$$
which is known as the ordinary least squares estimator, where 
$$
X=\left[\begin{array}{c}
-x_1- \\
\vdots \\
-x_n-
\end{array}\right] \quad y=\left[\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}\right]
$$
This estimator has some interesting properties. It is unbiased and has the minimal variance for all unbiased estimators (Gauss-Markov Theorem). % TODO what is this
Thus, from the formula
$$
\text{error} = \text{bias}^2 + \text{variance} + \text{noise}
$$
we find that this estimator is the one with the smallest error of all the unbiased estimators. Then why does no-one use this estimator, this is because if we introduce a bit of bias, we can significantly reduce the variance. So what is the variance of $\text{Var}(\hat b)$? For this we use the SVD of $X = UDV^\top$ if we plug this into the formula from above we get
$$\hat b = VD^{-1}U^\top y$$
so, since $y$ has a gaussian distribution and we just multiply it with a matrix ($VD^{-1}U^\top$), $\hat b$ also has a gaussian distribution. Then
$$\operatorname{Var}(\hat{\beta})=\ldots=\sigma^2 V D^{-2} V^{\top} =\sigma^2 \sum_{i \leq n} \frac{1}{D_{ii}^2} V_i V_i^{\top} $$
What are the implications of this? If we are working with high dimensional data, many of these features are typically correlated, this means that $X$ has a low rank, then the smallest $D_{ii}$ values are very small, which means that the bias is huge. 

\subsection{Act 2: Stability via Regularization}
The typical process of \textbf{Bayesian inference} goes through the follwing stages:
\begin{enumerate}
    \item Prior $\beta \sim \mathcal N(0, \tau^2 I)$
    \item Observe likelihood $p(D | \beta)$
    \item Posterior \textit{(adjustment of the likelihood)} $p(\beta | D) \propto p(\beta) p(D | \beta) \propto \exp \left(-\frac{1}{2 \sigma^2} \text{MSE}(D, \beta)-\frac{1}{2 \tau^2}\|\beta\|^2\right)$
\end{enumerate}
Observe that the last term is equivalent to the loss induced by the \textbf{Ridge}-Regression. (If we change $\beta$ to have a Laplace distribution (heavy tails), we get \textbf{Lasso}-Regression) By adjusting $\tau$ we can adjust the bias-variance tradeoff. If we do the math we get
$$\hat{\beta}_{\text {MAP }}=\left(x^{\top} x+\frac{\sigma^2}{\tau^2} I\right)^{-1} x^{\top} y$$ and if we do again an SVD to calculate the variance we get
$$\text{Var}(\hat \beta_{\text {MAP }}) = \sigma^2 \sum_{i\leq n} \frac{D_{ii}^2}{\left(D_i^2+\frac{\sigma_i^2}{\tau^2}\right)} V_i V_i^{\top}$$
So we can use $\tau$ to control the whole fraction term, which is an improvement to the situation we had before. 

\subsection{Act 3: Polynomial regression via }
Now we change our assumption for $f_*(x)$ so that it is a polynomial function on $x$. We have
$$f_*(x) = \varphi(x)^\top \beta _*$$
where $\beta_* \in \mathbb{R}^{\infty}$ and 
$$
\varphi(X)=K_x\left(\frac{x_1^{\alpha_1} ... x_d^{\alpha_d}}{\sqrt{\alpha_{1}!\ldots \alpha_{d}!}}\right)_{\alpha \in \mathbb{N}^d}
$$
Then for $x, x^{\prime} \in \mathbb{R}^a$
$$
\begin{aligned}
\varphi(x)^{\top} \varphi\left(x^{\prime}\right) & =K_{RBF}\left(x, x^{\prime}\right) \\
& =\exp \left(-\frac{1}{2} ||x-x^{\prime} ||^2\right)
\end{aligned}$$
and 
$$
\begin{aligned}
\hat{\beta} & =\arg \min _{\beta \in \mathbb{R}^{\infty}} \frac{1}{n} \sum_{i \leq n}\left(y_i-\varphi\left(x_i\right)^{\top} \beta\right)^2 \\
& =\Phi^{\top}\left(\Phi \Phi^{\top}\right)^{-1} y
\end{aligned}
$$
where 
$$
\Phi=\left[\begin{array}{l}
\varphi(x)^{\top}  \\
\varphi\left(x_2\right)^{\top}\\
\varphi\left(x_n\right)^{\top}
\end{array}\right] \in \mathbb R^{n \times \infty}
$$
This doesn't look good. But let $x_*$ be a test point. Then
$$
\begin{aligned}
x_x & =\varphi\left(x_*\right)^{\top} \hat{\beta} \\
& =\varphi\left(x_*\right)^{\top} \Phi^{\top}\left(\Phi \Phi^{\top}\right)^{-1}\\
&= k(x_*) K
\end{aligned}
$$
where $k(x_*) = \left(\varphi\left(x_*\right)^{\top} \varphi\left(x_i\right)\right)_{1 \leq n}$ and $K_{ij}=\left(\varphi(x_i)^{\top} \varphi(x_j)\right)$

The problem is that the inversion of the matrix is $O(n^3)$, which is a problem if our data are too high-dimensional.

\subsection{Act 4: Neural Networks}
We assume $f_*$ has only a single, very wide hidden layer.
$$f_*(X)=\frac{1}{\sqrt{m}} \sum_{i \leq m} \alpha_i \phi\left(\omega_i^{\top} X\right)$$
then $\theta=\left\{\alpha_i, w_i\right\}_{i \leq m}$. We use gradient descent. We initialize our NN with
$$
\left.\theta_0 \sim N\left(0, w^2\right]\right)
$$
and we update our parameters using gradient descent. 
$$
\left.\theta_{t+1} \leftarrow \theta_t-\eta \nabla_0 \text { MSE(D, } \theta_t\right)
$$
and we can calculate
$$\nabla_\theta \operatorname{MSE}\left(D, \theta_t\right)=\widetilde{\Phi}_t^{\top}\left(f_t-y\right)$$
where 
$$
\tilde{\Phi}_t=\left(-\nabla_0 f\left(x ; \theta_t\right)^\top-\right)_{i \leq n} \text{ and } f_t=(f(x ; \theta_t))_{i \in n}
$$
If we approximate $f_t$ using a first order taylor approximation
$$f_t \approx t_0+\tilde \Phi\left(\theta_t-\theta_0\right)$$
then
$$
\theta_t-\theta_0=\Phi^{\top}\left(\tilde{\Phi} \tilde{\Phi}^{\top}\right)^{-1}\left(f_t-f_0\right)
$$
Then let $x_*$ be a test point
$$
f_t\left(x_t\right) \approx f_0\left(x_k\right)+\nabla f\left(x_t, \theta_t\right)^{\top} \tilde{\Phi_t}^{\top}\left(\Phi_t \tilde{\Phi_t}^{\top}\right)^{-1}\left(f_t-f_0\right)
$$
if we then let $t$ and $m$ go $\infty$, this turns into
$$
f_t\left(x_t\right) \ = 0+\widetilde{K}\left(x_*\right)^{\top} K^{-1}(y-0)=k\left(x_*\right)^{\top} K^{-1} y
$$
This is exactly the result we also had at the end of act 3. \textbf{So gradient descent with NN is just simulating kernelized ridge-regression, except that we are using a different kernel.}